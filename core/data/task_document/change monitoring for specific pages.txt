name: Change monitoring for specific pages
description: Periodically check specific web pages for changes in selected content (elements, sections, or patterns), record differences, and keep simple text snapshots and logs in the workspace. Use an efficient, script-based approach with minimal extra steps.

goal/outcome:
  - For each monitored page in {urls}, the agent:
    - Fetches the current content.
    - Compares it against a stored baseline snapshot (if one exists) for the selected elements or text regions.
    - Detects whether relevant changes have occurred (changed/unchanged).
  - A single log file {change_log_file} is created or updated in {monitoring_root}, summarizing:
    - {url}, monitoring key (e.g., selector or pattern), status (new/baseline_created, changed, unchanged, error), and a brief change summary (e.g., counts or short diff snippet).
  - Baseline snapshot files are stored in a dedicated folder {baseline_dir}, such as one text file per {url_key}, to support future comparisons.
  - For changed pages, a current snapshot is saved (e.g., {snapshot_dir}/{url_key}_{run_timestamp}.txt) and a brief text diff is written to the log or a sidecar diff file.
  - The user receives a concise summary describing:
    - How many pages were checked.
    - How many showed changes vs. no changes.
    - Where baselines, snapshots, and logs are stored.
  - The task can be safely invoked again in the future using the same {monitoring_root} to continue monitoring periodically.

inputs_params:
  - Required: {urls} — list of page URLs to monitor.
  - Required: {monitoring_root} — base directory in the workspace where baselines, snapshots, and logs should be stored.
  - Required: {monitoring_targets} — per-page or global rules describing what to monitor, such as:
    - Simple text patterns to watch (e.g., strings that should be tracked or extracted).
    - Optional: tag/element hints (e.g., “main content”, “price section”) expressed as extra context for the script.
  - Optional: {page_id_strategy} — how to derive a stable identifier {url_key} for each URL (e.g., slug from URL, hash of URL).
  - Optional: {change_sensitivity} — how strict to be when detecting changes, for example:
    - ignore_whitespace_only_changes: true/false
    - ignore_case: true/false
  - Optional: {max_chars_per_page} — limit of characters per page to store/compare, to keep snapshots small.
  - Optional: {diff_mode} — e.g., "summary" (counts only) or "unified" (short unified diff snippet).
  - Optional: {run_label} or {run_timestamp_override} — if the caller wants a specific label/timestamp used in snapshot/log file naming.

context:
  reasoning:
    - Keep monitoring efficient and repeatable:
      - Run a single Python script that:
        - Fetches each {url},
        - Extracts or filters the content relevant to {monitoring_targets},
        - Compares it to an existing baseline (if present),
        - Writes results and updates baselines in one pass.
    - Use "create and run python script" rather than multiple network and file actions to reduce overhead.
    - For each {url}, derive a stable {url_key} (e.g., a sanitized version of the URL or a hash) and store:
      - Baseline: {baseline_dir}/{url_key}.txt
      - Latest snapshot: {snapshot_dir}/{url_key}_{run_timestamp}.txt
    - On the first run (no baseline file yet):
      - Create the baseline from the current content.
      - Log status as "baseline_created".
    - On subsequent runs:
      - Read the baseline and compute a simple diff (for example, using Python’s difflib) or a hash comparison.
      - If content is unchanged according to {change_sensitivity}, log "unchanged".
      - If content changed, log "changed" and optionally include a short diff snippet (limited to keep logs readable).
      - Optionally update the baseline automatically to the new version, so future runs compare against the latest known version.
    - Always keep logging simple and compact:
      - A single {change_log_file} per {monitoring_root}, with one row per {url} per run.
    - The environment or user can call this task periodically (e.g., via an external scheduler) to achieve recurring monitoring.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - For each {url} in {urls}:
      - The script either:
        - Created a baseline, or
        - Compared to an existing baseline and produced a "changed" or "unchanged" outcome, or
        - Recorded a clear error status.
    - {baseline_dir} exists and contains a baseline file for every successfully processed page.
    - {change_log_file} exists and includes at least one entry for this run, with:
      - url, url_key, status, and a brief detail field (e.g., "lines_changed=5").
    - The final user message states:
      - How many pages were processed.
      - How many were new, changed, unchanged, and errored.
      - Where baselines, snapshots, and logs are stored.
    - No unnecessary extra files or directories were created outside {monitoring_root}.
  avoid:
    - Using GUI mode or browser automation; HTTP-based fetching plus simple text processing is sufficient.
    - Over-scraping or doing complex DOM manipulation when simple text extraction is enough.
    - Storing excessive or unbounded content; respect {max_chars_per_page} if provided.
    - Running multiple separate scripts when one can handle fetch, compare, and log in a single pass.
    - Silently ignoring fetch errors; they must be logged with a status like "error_fetch".

steps:
  name: Confirm monitoring configuration
  description: Ensure the list of pages and how they should be monitored are clear before any network calls.
  action_instruction: >
    Use the "send_message" action to briefly summarize and confirm:
    - {urls}: list of pages to monitor.
    - {monitoring_root}: base folder for baselines, snapshots, and logs (e.g., {monitoring_root}/baselines, {monitoring_root}/snapshots, {monitoring_root}/logs).
    - {monitoring_targets}: what parts of the pages matter (e.g., “main body text”, “price block”, “specific phrase(s)”).
      This can be expressed as:
        - Text patterns to look for and capture (e.g., "product name", "price", "headline text"), or
        - Simple hints like "just use the main readable content".
    - Any {change_sensitivity} options (ignore whitespace-only changes, ignore case).
    - Any content limits ({max_chars_per_page}) to keep snapshots small.
    - Whether to automatically update the baseline when a change is detected (default: update baseline).
    If anything is unclear or missing (especially {urls} or {monitoring_root}), use "send_message" with a focused query.
    Once confirmed, send a short "send_message" recap so the configuration is explicit in the task history.
  validation_instruction: >
    Verify that:
    - {urls} is non-empty.
    - {monitoring_root} is specified.
    - {monitoring_targets} is at least minimally defined (even if it means "monitor overall page text").
    If any of these are missing or ambiguous, call "send_message" again and wait for a clear answer before proceeding.

  name: Run monitoring script (fetch, compare, update)
  description: Use a single Python script to fetch all pages, compare them to baselines, write logs, and maintain snapshots.
  action_instruction: >
    Use "create and run python script" with code that:
    - Uses only Python standard libraries (e.g., urllib/request or http.client, difflib, hashlib, os, json, datetime, pathlib).
    - Inputs (hard-coded or passed as simple variables in the script):
      - urls = {urls}
      - monitoring_root = {monitoring_root}
      - monitoring_targets = {monitoring_targets}
      - page_id_strategy = {page_id_strategy} or a simple default (e.g., URL-safe slug or hash).
      - change_sensitivity = {change_sensitivity} or defaults (e.g., ignore_whitespace_only_changes=False).
      - max_chars_per_page = {max_chars_per_page} or None.
      - diff_mode = {diff_mode} or default "summary".
    - Setup:
      - Define:
        - baseline_dir = os.path.join(monitoring_root, "baselines")
        - snapshot_dir = os.path.join(monitoring_root, "snapshots")
        - log_dir = os.path.join(monitoring_root, "logs")
      - Ensure these directories exist (os.makedirs(..., exist_ok=True)).
      - Create or append to a log file:
        - log_file = os.path.join(log_dir, "change_log_{run_timestamp}.csv") or a stable {change_log_file} with appended rows.
        - Write header if the file is new: ["timestamp","url","url_key","status","details"].
    - For each url in urls:
      - Derive url_key based on page_id_strategy (e.g., slugify or hash of url).
      - Fetch the page using urllib or requests-like logic:
        - Handle HTTP errors and timeouts gracefully.
        - If fetch fails, log status="error_fetch" with the error message, then continue to next URL.
      - From the fetched content:
        - If monitoring_targets specify text patterns, extract relevant portions:
          - Simple approach: keep full text, but optionally extract lines containing each pattern.
        - Normalize text according to change_sensitivity:
          - Optionally lower-case and/or collapse whitespace.
        - Truncate to max_chars_per_page if needed.
      - Compute baseline_path = os.path.join(baseline_dir, f"{url_key}.txt").
      - If baseline_path does not exist:
        - Write the normalized content to baseline_path.
        - Optionally also create an initial snapshot under snapshot_dir with {run_timestamp}.
        - Log a row: status="baseline_created", details="no previous baseline".
      - If baseline_path exists:
        - Read the baseline text.
        - Compare baseline and current content:
          - If identical under the chosen normalization, log status="unchanged".
          - If different:
            - Compute a simple summary (e.g., using difflib.ndiff to count changed lines, or hashes).
            - Optionally generate a short diff snippet if diff_mode == "unified".
            - Log status="changed", details="{summary of difference}" (e.g., "changed_lines=12").
            - Update baseline_path with the new content (if auto-update is enabled).
            - Save a snapshot in snapshot_dir with the current content and a timestamp in the filename.
    - After processing all urls:
      - Print to stdout a summary including:
        - total_urls
        - num_baseline_created
        - num_changed
        - num_unchanged
        - num_errors
        - the path of the log_file and baseline_dir/snapshot_dir.
  validation_instruction: >
    Check the script result:
    - status should indicate success (no unhandled exception).
    - stdout should show counts for baseline_created, changed, unchanged, and errors.
    - stdout should show the log file path and the baseline/snapshot directories.
    If the script fails globally, use "send_message" to briefly explain the failure and decide whether to retry
    with a fixed script or to "mark task error" if it cannot be resolved within reasonable effort.

  name: Overall validate monitoring run
  description: Verify that outputs align with the definition_of_done using the script summary and log file, without extra scans.
  action_instruction: >
    Perform a simple check using the information already produced:
    - Use "create and run python script" or "shell view" (if convenient) to:
      - Open the {change_log_file} mentioned in stdout.
      - Count how many rows correspond to this run (by timestamp or by file).
      - Confirm that the count matches total_urls (or total_urls minus any fetch errors).
      - Count how many entries have status in {"baseline_created","changed","unchanged","error_fetch"} or similar.
    - Ensure that:
      - Every url has at least one corresponding entry.
      - The number of "changed" and "error" entries is consistent with the summary from the monitoring script.
    This step should be quick and only rely on reading the log, not re-fetching pages.
  # No separate validation_instruction needed here; the above is sufficient.

  name: Close task and report to user
  description: Summarize the monitoring results, tell the user where artifacts are stored, and mark task status.
  action_instruction: >
    Use "send_message" to provide a concise summary including:
    - The number of pages processed ({total_urls}).
    - Counts:
      - baseline_created: {num_baseline_created}
      - changed: {num_changed}
      - unchanged: {num_unchanged}
      - errors: {num_errors}
    - The path to:
      - {baseline_dir}: where baseline files are stored.
      - {snapshot_dir}: where snapshots from this run were placed (if any).
      - {change_log_file}: log file summarizing this run.
    - A short explanation of what "changed" vs "unchanged" means under the chosen {change_sensitivity}.
    - A note that this same configuration can be reused for future periodic runs by calling this task again with the same {monitoring_root} and {urls}.
    If the run completed as expected and definition_of_done is met:
      - Call "mark task completed" with a brief message summarizing the main numbers and log locations.
    If the user decides to stop early or cancel:
      - Call "mark task cancel" with the reason.
    If a global script failure or other blocking error prevented meaningful monitoring:
      - Call "mark task error" with a short explanation and the log file path so the user can inspect partial results.
