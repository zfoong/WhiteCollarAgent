name: Table scraping from web pages
description: Detect HTML tables on one or more web pages and export their contents into CSV or spreadsheet formats, saving the result files in the agent workspace.

goal/outcome:
  - For each target URL, detect all HTML tables (or user-selected tables) and extract their contents.
  - Export extracted tables into one or more CSV or spreadsheet files in the workspace under {output_directory}.
  - Filenames follow a clear convention, for example:
    - {base_name}_table_{index}.csv for per-table exports, or
    - {base_name}_tables.{ext} for combined exports per page.
  - If a page contains no tables, this is recorded and reported clearly instead of failing silently.
  - A small machine-readable log file (e.g., CSV or .txt) is created listing:
    - {url}, table index/identifier, output file path, row/column counts, and status.
  - The user receives a concise summary: URLs processed, tables found per URL, formats used, and where outputs and logs are stored.

inputs_params:
  - Required: One or more target URLs, e.g., {url_list}.
  - Required: Output format choice:
    - "csv" (default), or
    - "xlsx" (spreadsheet), or
    - both.
  - Optional: Output directory in workspace, e.g., {output_directory} (default: ./tables_output_{date_time}).
  - Optional: Table selection rule when multiple tables are present, for example:
    - "all" (default),
    - specific indices (e.g., [0, 1]),
    - or a simple keyword filter to match table captions or nearby text.
  - Optional: Whether to export:
    - separate file per table, or
    - one combined file per URL.
  - Optional: Maximum number of tables per page to process (e.g., {max_tables_per_page}).
  - Optional: Whether to normalize header rows (e.g., use first row as header) and strip empty rows.
  - Optional: Request to add a column with the source URL to each exported table.

context:
  reasoning:
    - Keep the workflow short and efficient:
      - Confirm URLs and basic export preferences.
      - Run a single Python script to fetch pages, detect tables, and write CSV/XLSX outputs plus a log.
      - Do a quick sanity check on the script output and log, then report to the user.
    - Prefer "create and run python script" for the main work:
      - Use requests (or built-in urllib) to download each {url}.
      - Use a simple library-based approach (e.g., pandas.read_html) installed inside the script if needed.
      - Convert every detected table to a structured DataFrame.
    - Apply table selection rules:
      - If the user wants "all" tables, export them all.
      - If the user specifies indices or a simple keyword filter, only export those tables.
    - Export format logic:
      - CSV: df.to_csv({file_name}.csv, index=False).
      - XLSX: df.to_excel({file_name}.xlsx, index=False) using pandas + openpyxl if needed.
    - Record each processed table into a log for quick verification and potential reuse.
    - Use only as many actions as needed: one confirmation step, one script execution, one validation/summary step.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - Each URL in {url_list} has been processed:
      - At least one table exported, or
      - Explicitly logged as “no tables found”.
    - All requested formats (csv/xlsx) have been created under {output_directory} with consistent naming.
    - A log file exists at {log_file_path} with per-table entries: url, table_index, output_file, rows, columns, status.
    - No duplicate overwrites occurred unintentionally (files for different tables or URLs do not silently clobber each other).
    - The user has received a short summary indicating:
      - URLs processed,
      - tables per URL,
      - location of exported files and log,
      - any URLs with errors or no tables.
  avoid:
    - Overcomplicated scraping flows (e.g., multi-step headless browsing) when static HTML is sufficient.
    - GUI mode; use CLI and scripts only.
    - Deep custom HTML parsing when standard libraries can reliably detect tables.
    - Uncontrolled writes outside {output_directory}.
    - Failing silently when a page has no tables, returns an error, or has malformed HTML.

steps:
  name: Confirm scraping parameters
  description: Make sure URLs, output format, and basic table selection rules are clearly defined before scraping.
  action_instruction: >
    Use the "send_message" action to restate what is needed from the user and to confirm or fill in:
    - {url_list}: one or more web page URLs to scrape.
    - {output_format}: "csv", "xlsx", or "both" (default to "csv" if unspecified).
    - {output_directory}: where to store the exported tables (default to ./tables_output_{date_time}).
    - Table selection:
      - "all" tables (default), or
      - specific table indices (e.g., [0, 2]), or
      - a simple keyword filter to match table captions or nearby text (if they care).
    - Export granularity: separate file per table vs. combined file per URL.
    - Optional maximum tables per page (e.g., {max_tables_per_page}) to avoid exporting dozens of small tables unintentionally.
    - Whether to add a "source_url" column to each table.
    If any of these are missing or unclear (e.g., no URLs, or ambiguous format), use "send_message" to request a focused clarification.
    Once the user responds, send a short "send_message" summarizing the final agreed parameters so they are fixed in the task history.
  validation_instruction: >
    Check that:
    - {url_list} is non-empty and each {url} looks like a valid HTTP/HTTPS URL.
    - {output_format} is resolved to one of: "csv", "xlsx", or "both".
    - {output_directory} is defined (explicitly or default).
    - Table selection is at least "all" if the user gave nothing more specific.
    If any of these are missing or invalid, call "send_message" again with a narrow question and do not proceed until confirmed.

  name: Run table scraping and export script
  description: Fetch each URL, detect HTML tables, and export them into CSV/XLSX files plus a log in one efficient script.
  action_instruction: >
    Use "create and run python script" with a script that:
    - Takes as configuration (inline in code or via a small config structure):
      - url_list = {url_list},
      - output_directory = {output_directory},
      - output_format = {output_format},
      - table_selection (e.g., "all", indices, or keyword filter),
      - max_tables_per_page = {max_tables_per_page} (or None),
      - add_source_url_column = {add_source_url_column}.
    - Inside the script:
      - Create {output_directory} if it does not exist (os.makedirs).
      - Define a {log_file_path} inside {output_directory}, e.g. {output_directory}/tables_log_{date_time}.csv.
      - Import required libraries:
        - requests (if needed; can also use urllib), and
        - pandas (install via subprocess + pip if not already available) for read_html and CSV/XLSX export.
      - For each url in url_list:
        - Download the page HTML (with a reasonable timeout).
        - Handle HTTP or connection errors gracefully:
          - If an error occurs, write a log entry with status="fetch_error" and skip to the next URL.
        - Use pandas.read_html(html) to detect tables, catching exceptions if HTML is malformed.
        - If no tables are found:
          - Write a log entry for this URL with status="no_tables_found".
          - Continue to the next URL.
        - Apply table selection rules:
          - If "all": use all detected tables, possibly limited by max_tables_per_page.
          - If indices are provided: select only those indices that exist.
          - If a keyword filter is provided: keep only tables whose detected caption or nearby text (if available) matches the keyword (best-effort).
        - For each selected table (as a DataFrame df):
          - Optionally add a "source_url" column with the current URL.
          - Normalize header row if requested (e.g., first row as header, strip whitespace).
          - Decide on a base filename for this table, for example:
            - {sanitized_url_base}_table_{index}, where {sanitized_url_base} replaces non-alphanumeric characters with underscores.
          - For CSV export:
            - df.to_csv({output_directory}/{file_name}.csv, index=False).
          - For XLSX export:
            - df.to_excel({output_directory}/{file_name}.xlsx, index=False).
          - Append a row to the log with:
            - url, table_index, output_file_path(s), number of rows, number of columns, status="exported".
      - After processing all URLs:
        - Ensure the log file is written to disk.
        - Print to stdout a brief summary:
          - Number of URLs processed.
          - Total tables detected.
          - Total tables exported.
          - Number of URLs with no tables or fetch errors.
          - Path to {output_directory} and {log_file_path}.
  validation_instruction: >
    Inspect the "create and run python script" output:
    - Confirm that the script status indicates success (no unhandled exceptions).
    - Check stdout for:
      - A summary with counts (URLs processed, tables exported).
      - The {output_directory} and {log_file_path}.
    - If the script reports zero exported tables for all URLs:
      - Use "send_message" to inform the user that no tables were found or all fetches failed.
      - Ask whether they want to adjust URLs or stop, rather than proceeding as if successful.

  name: Validate exported files
  description: Perform a quick consistency check that exported files and the log exist and that counts look reasonable.
  action_instruction: >
    Use a lightweight check to avoid extra complexity:
    - If needed, call "create and run python script" again or use "shell view" to:
      - Verify that {output_directory} exists and contains at least the expected number of CSV/XLSX files when tables were reported as exported.
      - Open the first few lines of {log_file_path} to confirm it is readable and structured (e.g., header + rows).
      - Optionally count lines in the log (excluding header) to confirm that the number of entries matches the reported exported tables plus any fetch/no-table statuses.
    If there is a clear mismatch (e.g., summary says 10 tables exported but log has 0 entries):
      - Use "send_message" to inform the user that there is an inconsistency.
      - Optionally rerun the scraping step once with corrected code or parameters.
      - If the problem persists and is not easily fixable, plan to "mark task error" with a brief explanation.
  # No separate validation_instruction beyond this simple consistency check.

  name: Close task and report to user
  description: Summarize the scraping results, provide the output and log locations, and mark the task status.
  action_instruction: >
    Use "send_message" to provide a short, structured summary:
    - List {url_list} (or a truncated version if very long).
    - Indicate:
      - How many URLs were successfully fetched.
      - How many URLs had no tables.
      - Total number of tables exported.
    - State the {output_format} used ("csv", "xlsx", or both).
    - Give the path of {output_directory} and {log_file_path}.
    - Mention any notable issues (e.g., fetch errors, malformed HTML, or skipped tables due to selection rules).
    If tables were exported as expected and there are no major issues:
      - Call "mark task completed" with a concise message summarizing counts and paths.
    If the user chose to stop after discovering no tables or repeated fetch errors:
      - Call "mark task cancel" with a brief reason.
    If the task could not complete due to recurring script errors or inaccessible URLs:
      - Call "mark task error" with a short explanation and reference to {log_file_path} for inspection.
