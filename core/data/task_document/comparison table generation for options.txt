name: Comparison table generation for options
description: Aggregate attributes of items such as tools, products, or services into a clear, side-by-side comparison table. Keep the workflow short: confirm options and attributes, gather key data, build the table once, then optionally export it to a file.

goal/outcome:
  - A complete comparison table that lists {options} side by side, with {attributes} as rows and each option as a column.
  - Attributes are concrete and useful (e.g., price, features, limitations, platform, support, licensing, performance), not vague marketing phrases.
  - Data is sourced from either:
    - User input, or
    - A quick online lookup (docs, official sites, or trusted summaries), as requested by the user.
  - The table is shared with the user in an easy-to-read format (e.g., Markdown in chat).
  - Optional: the table is saved as a file (e.g., .md, .docx, or .pdf) in the workspace at {comparison_table_path}.
  - A short summary explains the main similarities/differences and where to find the saved file (if created).

inputs_params:
  - Required: List of options to compare (e.g., {option_A}, {option_B}, {option_C}).
  - Required: Target comparison attributes or dimensions if the user has them (e.g., {price}, {feature_set}, {limits}, {platforms}, {support}, {licensing}).
  - Optional: User’s primary decision goal (e.g., “cheapest”, “best for team of 5”, “privacy-focused”).
  - Optional: Data source preference:
    - User-provided information only, or
    - Quick online lookup (official docs/pages, simple search).
  - Optional: Output preference:
    - Table only in chat, or
    - Table plus a saved file ({output_format}: md / docx / pdf, at {comparison_table_path}).
  - Optional: Any hard constraints (e.g., must run on {os}, budget <= {budget}, self-hosted only) to highlight in the table.

context:
  reasoning:
    - Keep the process minimal:
      - Confirm which options and attributes matter.
      - Gather only the information needed for those attributes.
      - Build a single comparison table in memory and output once.
    - Use "send_message" and "send_message" for clarification when options or attributes are unclear.
    - If online information is needed for each option, prefer:
      - "google search" or "google search batch" to quickly fetch key details, or
      - "read web page from URL" when the user provides specific links.
    - Represent the table in a simple structure (e.g., a list of dicts) when building it via "create and run python script" if that’s convenient, then render as Markdown or a simple text table.
    - If a file output is requested, use:
      - "create text file" for Markdown/plain text tables, or
      - "create word file" / "create pdf file" for richer document formats, using the same table content.
    - Avoid extra passes or complex post-processing; once the table is generated and checked for obvious gaps, deliver it.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - All requested {options} appear as columns in the comparison table.
    - All key {attributes} are present as rows with at least a short value for each option (or clearly marked as “N/A” if information is not available).
    - The user’s stated goal/constraints (if any) are visible in either:
      - An attribute row (e.g., “Fits budget {budget}?”), or
      - A short summary below the table.
    - If a file output was requested, the file exists at {comparison_table_path} and contains the same table content.
    - The user receives a brief summary that highlights main differences and confirms how to access the table and any file created.
  avoid:
    - Overloading the table with marketing language or unexplained jargon.
    - Adding attributes the user did not ask for unless they are obviously standard (e.g., price, platform), and even then keep it minimal.
    - Running many separate searches per attribute when a single quick search per option is enough.
    - Creating multiple versions of the file output; one final version is sufficient unless the user asks otherwise.
    - Using GUI mode; CLI and script-based actions are enough.

steps:
  name: Confirm options and attributes
  description: Make sure the list of options and the comparison attributes are clearly defined before doing any data gathering.
  action_instruction: >
    Use "send_message" to restate the current understanding:
    - Options to compare: {option_list}.
    - Planned attributes (e.g., {price}, {feature_set}, {limits}, {platforms}, {support}, {licensing}, {notes}).
    - Whether the user wants:
      - User-provided info only, or
      - Quick online lookup for missing data.
    - Output preference:
      - Table in chat only, or
      - Table plus file ({output_format}) saved at {comparison_table_path}.
    If some parts are unclear (e.g., user only said “compare these tools” without attributes), use "send_message" to get a short list
    of most important attributes (for example, “Which 3–5 attributes matter most for your decision?”). After the user replies,
    send a brief "send_message" summarizing the final agreed options and attributes.

  validation_instruction: >
    Confirm that:
    - At least two options in {option_list} are defined for comparison.
    - There is at least one attribute to compare; ideally 3–7 focused ones.
    - Data source preference (user-only vs. quick online lookup) is known.
    - Output format preference is known or defaults to “table in chat only”.
    If any of these are missing, call "send_message" to clarify and do not proceed until they are resolved.

  name: Gather attribute data for each option
  description: Collect the needed attribute values for each option using either user input, online lookup, or a mix, according to the user’s preference.
  action_instruction: >
    If the user prefers user-provided info only:
      - Use "send_message" (and if needed, "send_message") to ask them to provide or confirm values for each {option} × {attribute} in a compact format
        (e.g., bullet list or simple lines).
    If the user allows online lookup:
      - Use "google search" or "google search batch" with queries like
        "{option} {attribute}" or "{option} features pricing" to gather concise facts from official or reputable sources.
      - For any specific URLs the user provides, use "read web page from URL" to extract main content.
    When it’s helpful and simple, use "create and run python script" to:
      - Store data in a simple structure, such as a list of records:
        [
          {"option": "{option_A}", "{attribute_1}": "...", "{attribute_2}": "..."},
          {"option": "{option_B}", "...": "..."}
        ]
    Focus on filling all cells with short, factual values. If something is unknown, use “N/A” or a short note like “info not clearly stated”.
  validation_instruction: >
    Quickly verify that:
    - Each option has at least some values filled for each attribute.
    - No option is completely missing data.
    - There are no obviously duplicated or contradictory values for the same attribute within a single option.
    If a critical attribute is missing for many options, send a short "send_message" to the user explaining this and ask whether to:
    - Keep the attribute with partial data, or
    - Drop the attribute from the comparison.

  name: Generate comparison table
  description: Convert the collected data into a single, side-by-side comparison table suitable for display in chat and optional file export.
  action_instruction: >
    Use either:
      - Direct formatting in the agent (no extra action), or
      - "create and run python script" to take the structured data and render a Markdown table.
    The table format should be:
      - Header row: | Attribute | {option_A} | {option_B} | ... |
      - Rows: one attribute per row, with each cell containing a concise value.
    Ensure consistent ordering:
      - Attributes in the order agreed with the user.
      - Options in the order provided by the user.
    Once the table string is prepared:
      - Use "send_message" to show the Markdown table directly to the user.
    If the user requested a file:
      - For Markdown or plain text:
        - Use "create text file" to save the table (and a short title/summary) at {comparison_table_path}.
      - For .docx:
        - Use "create word file" with a simple Markdown document containing the table.
      - For .pdf:
        - Either:
          - Use "create pdf file" directly with a Markdown string that includes the table, or
          - Use "create text file" or "create word file" then "convert_to_pdf" if that’s the preferred pattern.
  validation_instruction: >
    Check that:
    - The table includes all {options} as columns and all {attributes} as rows.
    - No option column is entirely empty.
    - The table renders syntactically correctly (aligned pipes and header separator) in Markdown.
    - If a file was created:
      - The action’s status indicates success.
      - The returned path matches {comparison_table_path} or is clearly communicated.

  name: Overall validate comparison result
  description: Make a quick overall check that the output matches the user’s request and is usable for decision-making.
  action_instruction: >
    Review the table and (if applicable) the file creation result:
    - Confirm that options and attributes match what the user confirmed earlier.
    - Check that any key user constraint (e.g., {budget}, {platform}, {self_hosted}) is represented either as a row or a note.
    - If a major mismatch is found (e.g., wrong option or missing attribute), correct the table with a single additional pass rather than starting over.
    No extra intensive validation is needed beyond this simple consistency check.

  name: Close task and report to user
  description: Summarize the result, highlight key insights, share the table/file location, and mark the task status.
  action_instruction: >
    Use "send_message" to provide:
    - A brief summary of the main differences/similarities between {options} based on the table (e.g., which is cheaper, which has more features, which fits {user_goal}).
    - Confirmation that the comparison table has been generated and shown.
    - If a file was created:
      - The exact path {comparison_table_path} and its format {output_format}.
    If the table and any requested file were generated successfully:
      - Call "mark task completed" with a short message referencing the number of options compared and the table/file location.
    If the user changed their mind midway or decided they no longer need the comparison:
      - Call "mark task cancel" with a short reason.
    If the process failed due to repeated errors (e.g., could not access requested resources or persistent file creation failures):
      - Call "mark task error" with a brief explanation and any partial table/location that does exist.
