name: Copying structured data into spreadsheets
description: Extract structured data from one or more documents or web pages and write it into a spreadsheet-friendly table (CSV or XLSX) in the workspace, using a small number of efficient CLI-based actions.

goal/outcome:
  - A structured table file (e.g., {output_path}/{file_name}.csv or {output_path}/{file_name}.xlsx) is created in the workspace.
  - The table has a clear header row with the agreed column names {columns_spec}.
  - Each row corresponds to one logical record extracted from the source documents/web pages.
  - All selected sources {sources_list} are processed once, and relevant records are captured without obvious omissions or duplicates (according to user rules).
  - The user receives a concise summary of:
    - Sources processed.
    - Column schema used.
    - Number of rows extracted.
    - Output file path.

inputs_params:
  - Required: List of data sources:
    - Local files in workspace, e.g., {source_file_paths} (.pdf, .docx, .txt, .md, etc.).
    - Web page URLs, e.g., {source_urls}.
  - Required: Target schema:
    - Column names and order, e.g., {columns_spec} = [id, name, date, amount, status].
    - Brief description of what each column should contain.
  - Required: Output file setting:
    - Output format: {output_format} in {"csv", "xlsx"} (default "csv").
    - Output path and base name, e.g., {output_directory}/{file_name}.
  - Optional: Extraction instructions:
    - How to recognize a “record” (e.g., table rows, bullet items, lines matching a pattern).
    - Which parts of the documents/pages to ignore (e.g., headers/footers, boilerplate text).
  - Optional: Handling rules:
    - Duplicate rule: {dedupe_rule} in {"keep_first", "keep_last", "allow_all"}.
    - Missing data rule: {missing_rule} (e.g., leave empty, fill default).
  - Optional: Volume/safety constraints:
    - Maximum number of rows {max_rows} to extract.
    - Maximum number of sources to process {max_sources}.

context:
  reasoning:
    - Keep the workflow simple: confirm schema and sources → read raw content → parse structured records in a single script → write the table.
    - Prefer CLI-based processing with "create and run python script" for parsing and table creation.
    - Use:
      - "read pdf file" and "read word file" when dealing with PDFs or DOCX documents already in the workspace.
      - "read web page from URL" or "send HTTP requests" for web pages, depending on what’s easier for the given task.
      - "download from url" if you need to pull documents into the workspace before parsing them.
    - Normalize data in Python: identify records, map fields to columns, apply simple cleaning (trimming whitespace, normalizing dates/amounts where easy).
    - Write CSV via Python’s csv module (simplest and universally spreadsheet-compatible), or install a library like openpyxl inside the script if the user explicitly wants XLSX.
    - Log basic stats (rows per source, total rows) either in stdout or in a small accompanying text file, but avoid heavy extra artifacts.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - The output file {output_spreadsheet} exists in the workspace and opens as a valid table (CSV/XLSX).
    - The header row matches the agreed {columns_spec} order.
    - The row count is non-zero (unless the user expected no matches) and consistent with the script’s summary.
    - The user-facing summary message clearly states:
      - Sources processed.
      - Output file path.
      - Number of rows extracted.
      - Any notable limitations (e.g., skipped fields, partial parsing).
  avoid:
    - Over-complicating extraction with many separate passes when one script can handle reading and structuring.
    - Relying on GUI/spreadsheet applications; file-based CSV/XLSX is enough.
    - Silent data loss: if something cannot be parsed into the schema, prefer to log or leave the cell blank (according to {missing_rule}) rather than dropping entire records without trace.
    - Implementing heavy, general-purpose scraping frameworks when a simple pattern/table-based approach is sufficient.

steps:
  name: Confirm sources and schema
  description: Make sure the list of sources and the target table schema are clearly defined and agreed with the user.
  action_instruction: >
    Use "send_message" to restate your understanding in a compact checklist:
    - Sources:
      - Files: {source_file_paths} (if known).
      - URLs: {source_urls} (if known).
    - Target columns and order: {columns_spec}.
    - Brief mapping: for each column, what data it should hold (e.g., "date": invoice date in YYYY-MM-DD).
    - Output format: {output_format} (default "csv").
    - Output location: {output_directory}/{file_name}.
    - Any simple extraction hints (e.g., “data is in an HTML table”, “each line is one record”, “look under heading ‘Transactions’”).
    - Optional rules: {dedupe_rule}, {missing_rule}, {max_rows}, {max_sources}.
    Ask the user to correct or fill in missing pieces. If a critical part is unclear (e.g., no column list, or ambiguous sources),
    use "send_message" with a focused prompt (e.g., "Please list the exact columns you want in the spreadsheet, in order.").
    Once the user responds, send one brief "send_message" summarizing the finalized parameters so they are recorded.
  validation_instruction: >
    Verify that:
    - At least one source is provided (file path or URL).
    - A non-empty {columns_spec} list is defined and ordered.
    - An output format and path are chosen (or a default is acceptable).
    If any of these are missing, use "send_message" again. Proceed only once the user has clearly confirmed sources, schema, and output path.

  name: Acquire and read source content
  description: Ensure all sources are accessible in the workspace or via HTTP, and read their content in a way suitable for parsing.
  action_instruction: >
    For each source:
    - If it is a local file path:
      - If the file is .pdf, call "read pdf file" with {file_path} to obtain its text/content structure.
      - If the file is .docx, call "read word file" with {file_path}.
      - If the file is a plain text or markdown file, you can read it via a simple "create and run python script" that opens it and prints or returns the content.
    - If it is a URL:
      - Use "read web page from URL" with {url} and {timeout}/{max_bytes} suitable for the page size to retrieve the main content in a markdown-like or text form.
      - If "read web page from URL" is not appropriate for the specific page, fall back to "send HTTP requests" and parse HTML inside a Python script instead.
    Aggregate the raw text (or structured content) from each source into a simple representation:
    - Either store each source’s content in a temporary text file {temp_text_file} using "create text file", or
    - Pass the content directly into a "create and run python script" call via variables if efficient.
    Prefer a minimal number of actions: batch multiple sources in one script where convenient.
  validation_instruction: >
    Check each read action’s status:
    - Ensure no critical source failed silently (e.g., unreadable file or HTTP error).
    - If one or more sources cannot be accessed, "send_message" briefly listing which ones failed and ask whether to:
      - Continue with the remaining sources, or
      - Stop and let the user fix access issues.
    Proceed to extraction only with the set of sources the user has agreed to process.

  name: Extract records and build the table
  description: Parse the raw content from all sources, extract records matching the schema, and write the spreadsheet file.
  action_instruction: >
    Use "create and run python script" to do the heavy lifting in one pass:
    - Inputs:
      - List of source contents/paths {processed_sources}.
      - Column definitions {columns_spec}.
      - Optional rules {dedupe_rule}, {missing_rule}, {max_rows}.
      - Output format {output_format} and output path {output_spreadsheet}.
    - Inside the script:
      1. Load each source’s text/content.
      2. For each source, parse records according to the user’s hints:
         - If the data is in a clear table (e.g., HTML <table> or structured blocks), iterate over rows/elements.
         - If the data is list-like (one record per line or block), split accordingly.
         - Use simple regex or string operations to extract fields into the target columns.
      3. Build an in-memory list of dicts or rows with keys matching {columns_spec}.
         - Apply trimming/normalization (e.g., strip whitespace, simple date normalization if straightforward).
         - If {dedupe_rule} is set, perform a basic de-duplication by a chosen key or full-row comparison.
         - Enforce {max_rows} if provided (stop adding new rows after that).
      4. Write the result:
         - If {output_format} == "csv":
           - Use Python’s csv module to write the header row and all data rows to {output_spreadsheet}.
         - If {output_format} == "xlsx":
           - Install an XLSX library (e.g., openpyxl) within the script (via subprocess pip install) and write an .xlsx file with a single sheet and header row.
      5. Print to stdout:
         - Number of sources processed.
         - Number of records extracted.
         - Output file path.
         - Any simple notes (e.g., number of duplicates skipped, number of rows with missing fields).
  validation_instruction: >
    From the action output:
    - Confirm the script completed without unhandled exceptions.
    - Confirm the reported record count is reasonable (non-zero unless expected).
    - Note the exact path of {output_spreadsheet}.
    If the script failed or produced zero rows unexpectedly, send a short "send_message" explaining what happened and, if needed,
    ask the user whether to adjust parsing rules or stop.

  name: Overall validate spreadsheet output
  description: Perform a quick final check that the spreadsheet is structurally correct and matches the agreed schema.
  action_instruction: >
    Use a lightweight check via "create and run python script" or "shell view" (for CSV) to:
    - Open {output_spreadsheet}.
    - Confirm:
      - The first row matches {columns_spec} exactly (names and order).
      - The file has at least one data row (unless the user expected no matches).
    - Optionally sample a few rows and verify that obvious fields (like IDs, dates, or amounts) look correctly placed and not clearly misaligned.
    If something looks clearly wrong (e.g., header mismatch or empty file), send a brief "send_message" to the user describing the issue and asking if they want to rerun extraction with adjusted rules.
  # No extra validation_instruction beyond these simple checks.

  name: Close task and report to user
  description: Summarize what was done, provide the output file path, and set task status.
  action_instruction: >
    Use "send_message" to provide a concise summary:
    - Sources processed: list of {sources_list} (or a brief description if many).
    - Columns used: {columns_spec}.
    - Output file: {output_spreadsheet} (format {output_format}).
    - Number of records extracted (and any duplicates skipped, if applicable).
    - Any notable limitations or assumptions (e.g., “records were taken only from the first table on each page”).
    If the spreadsheet looks correct and the extraction met the user’s expectations:
      - Call "mark task completed" with a short success message and the output path.
    If the user chose to stop early or not proceed after issues:
      - Call "mark task cancel" with a brief reason.
    If persistent errors meant that a usable spreadsheet could not be produced:
      - Call "mark task error" with a short explanation and any partial output file path, if it exists.
