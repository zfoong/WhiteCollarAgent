name: Automated web search and result collection
description: Perform web searches for one or more queries and collect titles, links, and snippets into a structured list, optionally saving the results to a file in the workspace.

goal/outcome:
  - All provided queries {queries} are searched on the web using the available search actions.
  - For each query, a list of up to {max_results_per_query} results is collected, including at least:
    - title
    - url
    - snippet or brief description
  - Results are organized by query in a single structured collection (e.g., list of objects or rows).
  - The collection is saved to a single output file in the workspace at {output_file_path} in the requested format (default: markdown or CSV).
  - A concise summary message is sent to the user indicating:
    - which queries were executed,
    - how many results were collected per query,
    - and where the output file is stored.

inputs_params:
  - Required: One or more search queries {queries} (list of strings).
  - Optional: Maximum number of results per query {max_results_per_query} (default: 10).
  - Optional: Output file format {output_format}, e.g., "markdown" or "csv" (default: "markdown").
  - Optional: Output file path {output_file_path}, e.g., "{workspace}/web_search_results_{date_time}.md".
  - Optional: Whether to deduplicate results across queries by URL {deduplicate_across_queries} (default: false).
  - Optional: A short note about the search purpose {search_purpose} to include in the header of the output file.

context:
  reasoning:
    - Keep the flow simple and efficient: confirm queries and parameters, perform searches in as few actions as possible, collect results, save them once, and report.
    - Prefer "google search batch" when multiple queries are provided to reduce calls and overhead.
    - Use "google search" for single-query scenarios if simpler, but it is acceptable to always use "google search batch" when queries are known as a list.
    - For each query, extract only the needed fields (title, url, snippet) from the search results; ignore extra metadata unless the user explicitly requests it.
    - Build a single in-memory structure like:
      - [{query: ..., title: ..., url: ..., snippet: ...}, ...]
      grouped or easily filterable by query.
    - Optionally deduplicate across queries by URL when {deduplicate_across_queries} is true, keeping the first occurrence and noting the queries that matched.
    - Use "create text file" once to write the final formatted list to {output_file_path} in markdown or CSV.
    - Use "send_message" once at the end to summarize outcome and share the file path; avoid unnecessary intermediate chatter.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - All queries confirmed with the user have been processed or clearly reported as failed.
    - For each query, up to {max_results_per_query} results were collected, unless fewer were available.
    - The output file {output_file_path} exists in the workspace and contains:
      - For each entry: query, title, url, and snippet.
      - Basic structure consistent with the chosen {output_format}.
    - The final "send_message" includes:
      - List of queries actually searched.
      - Count of results per query.
      - Path to the output file.
    - No critical errors remain unreported (e.g., network issues or empty results are mentioned clearly if they occur).
  avoid:
    - Running multiple redundant search calls when "google search batch" can handle all queries at once.
    - Adding complex extra processing (e.g., content scraping, deep analysis) unless explicitly requested.
    - Mixing other unrelated tasks (e.g., downloading pages, summarizing content) into this workflow.
    - Using GUI mode; CLI and the provided HTTP/search actions are sufficient.
    - Silently ignoring failed queries; each failure should be recorded in the output or reported in the summary.

steps:
  name: Confirm queries and parameters
  description: Ensure the list of queries and basic options (results limit, output format, file path) are clearly defined before searching.
  action_instruction: >
    Use the "send_message" action to restate the current understanding of the task:
    - Queries: {queries}
    - Max results per query: {max_results_per_query} (default 10 if not provided)
    - Output format: {output_format} ("markdown" or "csv"; default "markdown")
    - Output file path: {output_file_path} (default something like "web_search_results_{date_time}.md" in the workspace)
    - Deduplicate across queries by URL: {deduplicate_across_queries} (true/false; default false)
    Ask the user to confirm or adjust these values. If any required input (e.g., queries) is missing or unclear,
    use the "send_message" action with a short, focused question (e.g., "Please provide one or more search queries to run.").
    Once clarified, send one more brief "send_message" with the final confirmed parameters for the record.
  validation_instruction: >
    Confirm that:
    - There is at least one non-empty query string in {queries}.
    - {max_results_per_query} is a positive integer or can be set to a default (e.g., 10).
    - {output_format} is either "markdown" or "csv", or can be safely defaulted to "markdown".
    - {output_file_path} is a valid-looking path string in the workspace (or a reasonable default can be used).
    If any of these are missing or invalid, ask the user once more using "send_message" and update values before proceeding.

  name: Execute web searches and collect results
  description: Run the web searches for all confirmed queries and collect basic fields (query, title, url, snippet).
  action_instruction: >
    If there is more than one query in {queries}, use the "google search batch" action:
      - Set "queries" to {queries}.
      - Set "num_results" to {max_results_per_query}.
    If there is exactly one query, you may either:
      - Use "google search" with "query"={query} and "num_results"={max_results_per_query}, or
      - Use "google search batch" with a single-query list for consistency.
    From the action's "search_results" or "results" output, extract for each query:
      - query_text
      - result_title
      - result_url
      - result_snippet or brief description (if available)
    Build an internal list of entries of the form:
      - { "query": query_text, "title": result_title, "url": result_url, "snippet": result_snippet }
    If {deduplicate_across_queries} is true:
      - Perform a simple in-memory deduplication by url:
        - Keep the first occurrence of each url.
        - Optionally aggregate the list of queries that produced that url in a "queries" field.
    Keep the collection in memory, ready for serialization to file.
  validation_instruction: >
    Verify that:
    - The search action(s) returned without error; if an error is reported (e.g., network issue), send a short "send_message"
      explaining that searches failed and ask the user whether to retry or stop.
    - For each query, check that you have at least zero to {max_results_per_query} entries; zero is allowed but should be noted.
    - If all queries returned zero results, prepare to mention this in the final summary rather than continuing silently.

  name: Save results to output file
  description: Serialize the collected entries into a single output file in the requested format and store it in the workspace.
  action_instruction: >
    Prepare a string representation of the results based on {output_format}:
    - If "markdown":
      - Create a header with optional {search_purpose}, date/time {date_time}, and a short explanation.
      - For each query, include a section like:
        - "## {query}"
        - Then bullet points or a table listing "[title](url) â€” snippet".
    - If "csv":
      - Create a header row: "query,title,url,snippet".
      - For each entry, add one row; escape commas/newlines as needed.
    Use the "create text file" action to write this content to {output_file_path}.
    If the file already exists and the user has not given special instructions, overwriting is acceptable for this task.
  validation_instruction: >
    Confirm that:
    - The "create text file" action reports success (status indicates success).
    - The path returned matches {output_file_path} (or is clearly communicated).
    - The content length is non-trivial (not empty if there were any results).
    If file creation fails (e.g., invalid path), send a short "send_message" explaining the issue and, if reasonable,
    adjust the path (e.g., simplify the filename) and retry once before giving up.

  name: Overall validate search collection
  description: Perform a simple final check that searches ran, results were collected, and the output file is consistent with expectations.
  action_instruction: >
    Use minimal checks to avoid extra overhead:
    - Confirm the number of queries processed matches the size of {queries}.
    - Confirm that the total number of result entries matches what you expect from the search output (sum over queries after deduplication, if any).
    - Optionally, use a quick "create and run python script" or "shell view" to inspect the first few lines of {output_file_path}
      and ensure it looks structurally correct (markdown headings or CSV header present).
    If anything is clearly inconsistent (e.g., empty file despite non-empty results, or counts do not match), fix the serialization
    once (e.g., regenerate the file) and re-check. If it still fails, plan to report the issue to the user.

  name: Close task and report to user
  description: Summarize what was done, where results are saved, and mark the task status accordingly.
  action_instruction: >
    Use "send_message" to send a concise summary including:
    - The list of queries that were actually searched.
    - The value of {max_results_per_query}.
    - The total number of results collected (and, if easy, a per-query breakdown).
    - Whether deduplication across queries was applied.
    - The output format ({output_format}) and the exact {output_file_path} where results are stored.
    - A note if any queries returned zero results or if any search failures occurred.
    If the collection and file creation were successful:
      - Call "mark task completed" with a short message such as
        "Web search and result collection completed; results saved to {output_file_path}."
    If the user canceled the task before searches were run or results were saved:
      - Call "mark task cancel" with a brief reason.
    If persistent errors prevented successful searches or file creation:
      - Call "mark task error" with a short explanation and any partial file path if available.
