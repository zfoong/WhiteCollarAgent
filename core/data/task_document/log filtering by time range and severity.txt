name: Log filtering by time range and severity
description: Filter one or more log files to only show entries within a specified time window and severity levels (e.g., ERROR, WARN). Use a small number of efficient CLI-based actions and write the filtered results to a separate output file in the workspace.

goal/outcome:
  - Only log entries within {start_time}–{end_time} (or {relative_time_window}, e.g., "last 2 hours") are selected.
  - Only log entries whose severity matches the requested levels (e.g., {severity_levels} such as ERROR, WARN, INFO) are retained.
  - A new filtered log file {output_log_file} is created in the workspace containing the matching entries.
  - The original log files are left unchanged.
  - A short summary is sent to the user: which files were processed, which time range and severity levels were used, how many lines matched, and where {output_log_file} is located.

inputs_params:
  - Required: One or more log sources:
    - A directory path {log_root_directory} containing log files, and/or
    - Specific log files {log_file_paths} if the user wants to target them directly.
  - Required: Time filter:
    - Exact start and end timestamps: {start_time}, {end_time}, or
    - A relative window (e.g., {relative_time_window} such as "last 1 hour", "today").
  - Required: Severity filter:
    - One or more severity levels (e.g., {severity_levels} such as ["ERROR", "WARN"]).
  - Optional: Log timestamp format pattern {timestamp_format}, e.g., "%Y-%m-%d %H:%M:%S".
  - Optional: Log severity extraction rule:
    - Field/keyword patterns (e.g., severity appears after timestamp in square brackets).
  - Optional: File selection rules under {log_root_directory}:
    - File name pattern(s) such as "*.log", "*.out" ({log_file_pattern}).
  - Optional: Output file name/path {output_log_file}, defaulting to something like "filtered_logs_{date_time}.log" in the workspace.
  - Optional: Maximum number of lines to process (safety cap) {max_lines}.

context:
  reasoning:
    - Keep the workflow lean: confirm parameters, pick files, run one Python script to filter, then report.
    - Prefer using "create and run python script" to:
      - Enumerate candidate log files (if given a directory).
      - Parse each line, extract timestamp and severity, and apply filters in a single pass.
    - Use simple, user-specified or default assumptions for log format (e.g., first token is timestamp, a known {timestamp_format}; severity appears as a token like "ERROR", "WARN").
    - Avoid editing original logs; always write the filtered result to a new file.
    - Keep logging simple: only the filtered output file plus a brief count summary in stdout.
    - Use "shell view" only if needed for a quick preview of the filtered file; do not add extra passes.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - At least one log file was processed successfully.
    - A filtered log file {output_log_file} exists and is readable.
    - The filtered file contains only lines whose timestamps fall within the requested time range and severities match {severity_levels}, according to the chosen parsing rules.
    - The user receives a concise summary describing:
      - Which log files were included.
      - The time window and severity levels used.
      - Total lines scanned and total lines written.
      - The path of {output_log_file}.
  avoid:
    - Modifying or truncating original log files.
    - Overly complex parsing logic; stick to one clear timestamp/severity parsing strategy per run.
    - Multiple redundant scans of the same logs.
    - Assuming a specific log format without confirming or at least stating the assumption to the user.
    - Scanning unrelated directories or files outside the provided scope.

steps:
  name: Confirm log scope and filter criteria
  description: Make sure the agent understands what logs to look at, which time range to filter on, and which severity levels to include.
  action_instruction: >
    Use the "send_message" action to clearly restate and confirm:
    - The log location:
      - A root directory {log_root_directory}, and/or
      - Specific log files {log_file_paths}.
    - A file pattern {log_file_pattern} for selecting files under {log_root_directory} (e.g., "*.log").
    - The time filter:
      - Exact {start_time} and {end_time}, or
      - A relative window {relative_time_window}. If relative, state how it will be interpreted (e.g., "last 2 hours from current time").
    - The severity filter {severity_levels} (e.g., ["ERROR", "WARN"]).
    - The assumed log timestamp format {timestamp_format}, for example "%Y-%m-%d %H:%M:%S", and where it appears in the line (e.g., at the beginning).
    - How severity will be detected (e.g., as a token like "ERROR" or "[ERROR]" in each line).
    - The desired output file path or name {output_log_file}, or that a default will be used.
    Ask the user to correct or fill in any missing critical pieces. If a key detail is still unclear
    (e.g., timestamp format, or how to interpret the time range), use "send_message" with a focused question such as
    "What timestamp format does your log use?" Once clarified, send a short "send_message" summarizing the final parameters.
  validation_instruction: >
    Verify that:
    - There is at least one known log source (directory and/or explicit file list).
    - A valid time filter is defined (either specific {start_time}/{end_time} or a clear {relative_time_window}).
    - At least one severity level is specified in {severity_levels}.
    - A timestamp format {timestamp_format} is provided or a reasonable default is agreed upon.
    If any mandatory element is missing, repeat a targeted "send_message". Proceed only once the user confirms the final settings.

  name: Identify log files to process
  description: Determine which log files to read based on the confirmed directory, file patterns, and explicit file paths.
  action_instruction: >
    Use "create and run python script" to:
    - Accept:
      - {log_root_directory} (may be None),
      - {log_file_pattern} for matching files under the directory (e.g., "*.log"),
      - {log_file_paths} list provided explicitly by the user (if any).
    - Initialize a list of files to process.
    - If {log_root_directory} is provided:
      - Check if it exists and is a directory.
      - Use os.walk (or a simple listdir if the user intends only top-level) to find files matching {log_file_pattern}.
      - Append matching absolute paths to the file list.
    - Add any explicit {log_file_paths} to the list (if they exist).
    - Remove duplicates from the list.
    - Print to stdout:
      - Total number of log files found.
      - A short sample list (e.g., first up to 5 paths).
    Return status, stdout, and stderr via the action output.
  validation_instruction: >
    Check that the script ran successfully and that:
    - At least one log file path was found and listed in stdout, unless user expects zero.
    - If zero files are found, send a "send_message" explaining that no logs matched the pattern or paths,
      and ask if they want to adjust {log_root_directory}, {log_file_pattern}, or {log_file_paths}.
    Proceed only when there is a non-empty list of log files to filter.

  name: Filter logs by time range and severity
  description: Read the selected log files and write only matching lines (time range + severity) into a new filtered output file.
  action_instruction: >
    Use "create and run python script" with a script that:
    - Accepts:
      - The list of log file paths from the previous step.
      - {start_time} and {end_time} (converted to datetime), or a relative window {relative_time_window} interpreted using the current time.
      - {timestamp_format} describing how to parse timestamps from each line.
      - {severity_levels} list (e.g., ["ERROR", "WARN"]).
      - Output file path {output_log_file} in the workspace.
      - Optional {max_lines} safety cap.
    - Convert the provided time filter into concrete datetime objects:
      - If {relative_time_window} is used, compute {start_time} and {end_time} using datetime.now().
    - Open {output_log_file} for writing (create/overwrite).
    - For each log file:
      - Open it in text mode with a reasonable encoding (e.g., "utf-8" with errors="ignore").
      - Stream through each line:
        - Attempt to parse the timestamp from the line using {timestamp_format}. For example, parse the substring at the start of the line.
        - If parsing fails, optionally skip the line or treat it as "no timestamp"; in either case, do not include it in the filtered output.
        - If timestamp is parsed, check whether {start_time} <= timestamp <= {end_time}.
        - Check whether any of the severities in {severity_levels} appears in the line (e.g., contains "ERROR" or "WARN").
        - If both the time window and severity conditions are satisfied, write the line to {output_log_file}.
        - Track counters: total lines read, lines matched, lines skipped, and parsing failures.
        - If {max_lines} is set and the total lines read exceeds this cap, stop processing further lines and record that the cap was reached.
    - At the end:
      - Close the output file.
      - Print to stdout:
        - Total files processed.
        - Total lines read.
        - Total lines written to {output_log_file}.
        - Number of lines skipped due to parsing errors or being out of range.
        - Whether {max_lines} was reached.
        - The path to {output_log_file}.
  validation_instruction: >
    Check that:
    - The script status indicates success with no unhandled exception.
    - stdout shows non-zero "lines read" and either zero or more "lines written" as expected.
    - The output path {output_log_file} is clearly mentioned.
    If "lines written" is zero, send a short "send_message" explaining that no entries matched the time/severity filter and asking
    whether the user wants to adjust {start_time}, {end_time}, {relative_time_window}, {severity_levels}, or {timestamp_format}.

  name: Overall validate filtered output
  description: Perform a minimal but sufficient check that the filtered file is present and structurally reasonable.
  action_instruction: >
    Optionally use "shell view" on {output_log_file} to:
    - Inspect the first few lines and ensure they look like valid log entries.
    - Confirm that the file is not unexpectedly empty (unless the user expected no matches).
    Alternatively, use a small "create and run python script" to:
    - Count the number of lines in {output_log_file}.
    - If non-zero, parse the first and last lines' timestamps using {timestamp_format} and confirm
      they fall within {start_time}–{end_time} (best-effort check).
    If something looks clearly wrong (e.g., wildly out-of-range timestamps or completely malformed lines),
    mention it in the final user message, but do not overcomplicate additional passes unless requested by the user.

  name: Close task and report to user
  description: Summarize the filtering results to the user and mark the task status.
  action_instruction: >
    Use "send_message" to give a concise summary including:
    - The log scope (directory {log_root_directory}, file pattern {log_file_pattern}, and any specific {log_file_paths}).
    - The time filter used (final concrete {start_time}–{end_time}).
    - The severity filter {severity_levels}.
    - How many files were processed.
    - Total lines read vs. total lines written into {output_log_file}.
    - Any notable parsing issues or if {max_lines} was reached.
    - The exact path to {output_log_file} in the workspace.
    If the filtering worked as expected, call "mark task completed" with a short message summarizing the key numbers and the output file path.
    If the user decides to stop after discovering no matches or incorrect format and does not want to adjust parameters, call "mark task cancel" with a brief reason.
    If the script could not process the logs at all due to persistent errors (e.g., unreadable files or invalid paths), call "mark task error" with a short explanation and any partial output file path if available.
