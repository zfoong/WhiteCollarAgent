name: Duplicate file detection and consolidation
description: Detect duplicate or simple near-duplicate files under one or more root directories, then move or delete extra copies according to user-defined rules, using a single efficient CLI-based workflow.

goal/outcome:
  - Duplicate (and optionally simple near-duplicate) files are identified under the specified root directory tree(s).
  - For each duplicate group, exactly one file is kept as the canonical copy according to user-defined rules.
  - Extra copies are either moved to a consolidation/quarantine folder or deleted, as requested by the user.
  - No non-duplicate files are deleted or moved by mistake.
  - A single log file is created in the workspace listing, per group:
    - Group id, canonical path, duplicate paths, and action taken per file.
  - The user receives a concise summary of:
    - Scope (directories scanned).
    - How duplicates were defined.
    - How many groups and files were affected.
    - Which actions (move/delete) were taken.
    - Where the log (and any consolidation folder) is located.

inputs_params:
  - Required: One or more root directory paths to scan for duplicates.
  - Required: Duplicate definition:
    - At minimum: “exact duplicates” as same file size + content hash (e.g., SHA-256).
    - Optional: “near-duplicate” rule, limited to simple logic such as:
      - Same size and extension with very similar names (e.g., case or small punctuation differences).
  - Required: Consolidation rule:
    - How to choose which copy to keep within a duplicate group (e.g., keep the first found, keep newest, keep oldest, keep the one in a preferred directory).
  - Required: Action for non-canonical copies:
    - Move to a consolidation/quarantine folder (preferred default).
    - Delete (permanent removal).
  - Optional: Consolidation folder path (for moved duplicates; default: a new folder under a chosen root or workspace).
  - Optional: Whether to include subdirectories (recursive: true/false).
  - Optional: Simple include filters (e.g., by extension: ".jpg", ".pdf") or size thresholds (e.g., skip files below X bytes).
  - Optional: Dry-run flag (detect and plan only; no move/delete).
  - Optional: Maximum number of files to scan (safety cap).

context:
  reasoning:
    - Keep the workflow as lean as possible: collect parameters, scan files and compute hashes, group duplicates, decide canonical files, then move/delete extras in a single or minimal number of scripts.
    - Use CLI-based operations with "create and run python script" for scanning, hashing, grouping, and file operations.
    - Define duplicates primarily via exact size + hash; support a simple near-duplicate heuristic only if requested to avoid complex/slow similarity algorithms.
    - Prefer moving duplicates to a consolidation folder by default rather than deleting, unless the user explicitly asks for delete.
    - Record all decisions (groupings and actions) in a single log file to allow the user to inspect and, if needed, manually restore files.
    - Use a dry-run when requested so the user can review groups and planned actions before making changes.
    - Perform only a simple final validation: basic counts and spot-checking via the log; avoid heavy extra passes.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - All specified root directories have been scanned (or skipped with a logged reason).
    - Duplicate groups are identified consistent with the defined rules (exact or simple near-duplicate).
    - For each group, exactly one canonical file remains in place (or at the chosen destination), and the others have either been moved or deleted as requested.
    - No non-duplicate files were moved/deleted.
    - A log file exists with group ids, canonical paths, duplicate paths, and actions per file.
    - The user has received a clear summary of:
      - Duplicate definition used.
      - Number of duplicate groups and files processed.
      - How many were moved versus deleted (if deletion was enabled).
      - Where the log (and consolidation folder, if any) is located.
  avoid:
    - Using GUI mode; this task is fully achievable via CLI.
    - Implementing heavy or opaque “near-duplicate” logic (image/audio similarity, fuzzy content) that is slow or token-expensive.
    - Deleting files without a clear user confirmation of the delete policy.
    - Scanning outside the specified root directories.
    - Running multiple redundant full scans when a single pass with hashing is sufficient.

steps:
  name: Confirm scope and consolidation rules
  description: Clarify and confirm which directories to scan, how duplicates are defined, and how extras should be handled.
  action_instruction: >
    Use the "send_message" action to restate the task in concrete terms and ask the user to confirm or fill in:
    - Root directory path(s) to scan.
    - Whether to include subfolders (recursive true/false).
    - Duplicate definition:
      - Default proposal: “exact duplicate” = same size + SHA-256 hash.
      - Ask if they also want a simple near-duplicate rule (e.g., same size and extension with slightly different names).
    - Consolidation rule for choosing which copy to keep:
      - For example, “keep newest by modification time”, “keep oldest”, or “keep the one in directory X”.
    - Action for non-canonical copies:
      - Move to a consolidation/quarantine folder (default suggestion).
      - Delete (only if explicitly requested).
    - Consolidation folder path (if moving), or ask whether to create a new subfolder (e.g., "_duplicates") under a root.
    - Optional filters:
      - Include extensions (e.g., only ".jpg" and ".png") or minimum file size.
    - Whether the user wants a dry-run (detect and log duplicates only, no file changes) before applying move/delete.
    - Optional safety cap on max number of files to scan.
    If any of these are unclear or conflicting (e.g., delete requested but no confirmation), use "send_message" with a focused question. Once clarified, send a short "send_message" summarizing the final agreed parameters for the task history.
  validation_instruction: >
    Ensure that:
    - At least one root directory path string is provided.
    - A clear duplicate definition is chosen (exact, and optionally simple near-duplicate).
    - A consolidation rule for choosing the canonical file is specified.
    - The action for non-canonical files is explicit (move vs delete).
    - If move is chosen, a valid-looking consolidation folder path or strategy is defined.
    If any of these are missing, call "send_message" again and do not proceed until the user has confirmed them.

  name: Scan and hash files to identify duplicate candidates
  description: Perform a single efficient traversal to collect files under the chosen roots, filter them, and compute hashes to find exact duplicate groups (and simple near-duplicates if requested).
  action_instruction: >
    Use "create and run python script" with a compact script that:
    - Accepts:
      - The list of root directories.
      - The recursion flag.
      - Include filters (extensions) and size thresholds.
      - Optional simple near-duplicate mode flag.
      - A maximum file count (if provided).
    - For each root:
      - Check if it exists; if not, record an error in stdout and skip that root.
    - Traverse directories using os.walk when recursive, or os.listdir for non-recursive.
    - Apply filters (e.g., extension, size) so only relevant files are included.
    - For each included file:
      - If the safety cap is reached, stop further collection.
      - Compute a content hash (e.g., SHA-256) in a buffered way (read in chunks) and record:
        - path, size, hash, modification time.
    - Group files by (size, hash) to produce exact duplicate groups.
    - If near-duplicate mode is enabled:
      - Within each size group, optionally create subgroups with similar file names (e.g., same extension and case-insensitive name ignoring underscores/hyphens).
    - Print to stdout:
      - Total files scanned.
      - Number of exact duplicate groups (with group sizes).
      - Number of near-duplicate groups (if enabled).
    - Write a simple intermediate data file (e.g., CSV or JSON) in the workspace listing all files with their group id, hash, size, mtime, and path.
  validation_instruction: >
    Check the action result:
    - Status indicates success and stderr does not contain unhandled exceptions.
    - At least one root was valid; if none were valid, notify the user via "send_message" and stop.
    - Total files scanned is reasonable (non-zero unless the user expects otherwise).
    - There is a non-zero number of duplicate groups if the user requested duplicates and they are expected.
    If no duplicates are found, send a "send_message" explaining that no duplicate groups were detected, and ask the user whether to adjust filters or end the task.

  name: Decide canonical files and planned actions per group
  description: Use the scan results to choose which file to keep in each duplicate group and which to mark as extra, based on user rules.
  action_instruction: >
    Use "create and run python script" with a short script that:
    - Reads the intermediate data file produced by the previous step.
    - For each duplicate (or near-duplicate) group:
      - Apply the user’s consolidation rule to select a canonical file:
        - Example: choose the file with the newest modification time, or oldest, or one located under a preferred directory prefix.
      - Mark all other files in the group as duplicates of that canonical file.
      - For each extra file, compute the planned target path:
        - If "move" is selected:
          - Place it under the consolidation folder, optionally preserving some directory structure or using group-based subfolders.
        - If "delete" is selected:
          - Plan action = "delete", and leave target path field empty or same as original for logging.
    - Write a consolidation plan file (e.g., CSV/JSON) in the workspace with, for each file:
      - group_id, canonical (true/false), original_path, planned_action (keep/move/delete), target_path (if move).
    - Print a concise summary to stdout:
      - Number of groups.
      - Total files.
      - Counts of planned keep/move/delete actions.
  validation_instruction: >
    Confirm:
    - The script completed successfully and produced the consolidation plan file.
    - For each group, exactly one canonical file is flagged (you can spot-check via "shell view" or by trusting the script’s summary).
    - Planned actions match the user’s choices (e.g., if user chose move-only, ensure delete count is zero).
    If a dry-run was requested:
      - Explain via "send_message" that a plan has been computed and share counts.
      - Ask if the user wants to proceed with applying the plan or adjust rules.

  name: Apply consolidation actions (move/delete extras)
  description: Execute the consolidation plan by moving or deleting non-canonical duplicates and logging all results.
  action_instruction: >
    If dry-run mode is enabled and the user wants to keep it as a preview only:
      - Skip this step and go directly to summary and completion, clearly stating that no files were changed.
    Otherwise, use "create and run python script" with code that:
      - Reads the consolidation plan file.
      - Prepares a log file path in the workspace (e.g., "duplicate_consolidation_log_{timestamp}.csv").
      - For each entry:
        - If action is "keep": do nothing but log status "kept".
        - If action is "move":
          - Ensure the target directory exists (create directories as needed).
          - Attempt to move the file (e.g., with os.rename or shutil.move) and catch any exceptions.
          - Log status "moved" or "move_error" with an error message if any.
        - If action is "delete":
          - Attempt to remove the file with os.remove, catching exceptions.
          - Log status "deleted" or "delete_error" with an error message if any.
      - After processing all entries, write the log file with columns such as:
        - group_id, canonical, original_path, action, target_path, status, error_message.
      - Print summary totals to stdout: kept, moved, deleted, and error counts, and the log file path.
  validation_instruction: >
    Inspect the script result:
    - Confirm it finished without unhandled exceptions.
    - Note the counts of moved/deleted/error from stdout.
    - Ensure the log file path is present in stdout and that the path looks valid.
    If error counts are significant, prepare to mention them explicitly in the final summary so the user can review and fix specific cases if needed.

  name: Final validation and report to user
  description: Perform a simple final check based on logs and provide the user with a clear summary, then mark the task status.
  action_instruction: >
    Use either the consolidation script’s stdout or, if needed, a quick "create and run python script" or "shell view" call to:
    - Confirm that:
      - The total number of processed file entries in the log matches the counts reported earlier.
      - Status values are limited to expected types (kept, moved, deleted, *_error).
    Then use "send_message" to send a concise summary:
      - Root directories scanned.
      - Duplicate definition used (exact + any near-duplicate rule).
      - Consolidation rule (how canonical files were chosen).
      - Whether this run was dry-run only or actually moved/deleted files.
      - Counts: groups found, total files in groups, numbers kept/moved/deleted, and error count.
      - The path to the consolidation log file.
      - The consolidation folder path, if move was used.
    Based on the outcome:
      - If everything is consistent and errors are minor or expected, call "mark task completed" with a brief description of success.
      - If the user canceled or chose to stop after a dry-run without applying changes, call "mark task cancel" with a short reason.
      - If the task encountered unrecoverable errors (e.g., many operations failed due to permissions), call "mark task error" with a clear message and reference to the log for further manual investigation.
