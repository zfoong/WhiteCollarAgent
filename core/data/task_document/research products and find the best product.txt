name: Research products and find the best product
description: Visit multiple product pages, record prices, reviews, availability, and key attributes into a simple table, then compare them to recommend the best product balancing quality and price.

goal/outcome:
  - A short, clearly defined product scope, such as "{product_category} for {use_case}" and basic constraints (e.g., {max_budget}, {preferred_brands}, {region}).
  - A set of {num_products} candidate products collected from multiple sources (e.g., online stores, comparison sites).
  - A structured comparison table saved in the workspace at {output_table_path} with one row per product and columns including at least:
    - product_name
    - source_url
    - price_value
    - price_currency
    - average_rating
    - rating_count (if available)
    - availability_status
    - key_attributes (short text)
  - A clear recommendation of a single “best” product (or a small top-3 list) based on quality/price tradeoff and user criteria.
  - A concise summary message to the user explaining:
    - Which products were considered.
    - How they were compared.
    - Why the recommended product(s) is/are preferred.

inputs_params:
  - Required: Product scope:
    - {product_category} (e.g., "wireless earbuds", "27-inch monitor").
    - Optional {use_case} (e.g., "for travel", "for office coding").
  - Required: Budget and constraints:
    - {max_budget} or budget range (e.g., 100–200 in {currency}).
    - {currency} (e.g., USD, EUR, JPY) if relevant.
  - Optional: Preference hints:
    - {preferred_brands} and/or brands to avoid.
    - {min_review_score} (e.g., minimum average rating, 1–5).
    - {min_review_count} (e.g., only consider products with ≥ 50 reviews).
  - Optional: Availability constraints:
    - {shipping_region} or country.
    - Whether only in-stock items should be considered.
  - Optional: Comparison scope:
    - Approximate {target_num_products} to compare (e.g., 5–10).
    - Sites to prioritize or avoid (e.g., Amazon, local retailers).
  - Optional: Output format:
    - {output_table_format}: "csv" or "markdown" table (default: "csv").
    - {output_table_path}, default something like "workspace/product_research_{product_category}_{date}.csv".

context:
  reasoning:
    - Keep the workflow compact: confirm scope and criteria, find candidates with a small number of searches, extract basic structured fields, then rank.
    - Use "google search" or "google search batch" to quickly find relevant product pages based on {product_category}, {use_case}, and {shipping_region}.
    - Prefer a single "create and run python script" to:
      - Take a list of product URLs (from previous search results).
      - Fetch each page using Python standard libraries (e.g., urllib).
      - Extract approximate product name, price, rating, review count, and availability using simple pattern matching on HTML text.
      - Normalize prices into a consistent numeric field if possible (price_value and price_currency).
      - Save a comparison table to {output_table_path} in the requested format.
    - Apply simple filters (e.g., drop items above {max_budget}, below {min_review_score}, or with very few reviews).
    - Rank remaining products using a straightforward scoring heuristic (e.g., higher rating and review count, lower price).
    - Favor clarity and reliability over overly complex parsing: if some fields are missing for a product, record blanks but still include it.
    - Keep messaging to the user short and actionable; avoid unnecessary extra passes.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - At least {min_candidates} viable products (e.g., ≥ 3) are included in the comparison table, or the user is clearly told if fewer are available.
    - The comparison table file at {output_table_path} exists in the workspace and has one row per considered product with the key columns populated where possible.
    - Prices and ratings look reasonable (no obviously bogus values like negative prices or 100/5 ratings).
    - A recommendation is clearly stated and justified against the user’s criteria (budget, rating preferences, etc.).
    - The final "send_message" to the user includes:
      - The recommended product (and maybe 1–2 strong alternatives).
      - Basic reasoning (price vs rating vs key attributes).
      - The path to the table for deeper review.
  avoid:
    - Visiting an excessive number of pages; limit to a reasonable amount (e.g., first {search_limit} relevant results from 1–3 queries).
    - Trying to perfectly parse every site’s detailed specification; focus on core fields: price, rating, availability, and a few attributes.
    - Overfitting a complex scoring model; a simple, transparent heuristic is better.
    - Ignoring the user’s budget or region when selecting candidates.
    - Making claims not supported by extracted data (e.g., “best durability” without any evidence in reviews/specs).

steps:
  name: Confirm research scope and criteria
  description: Clarify what type of product the user wants, their budget, and how they define “best” so that search and comparison remain focused.
  action_instruction: >
    Use the "send_message" action to summarize and confirm:
    - {product_category} and optional {use_case}.
    - {max_budget} and {currency}, or state that there is no strict budget.
    - Preference hints: {preferred_brands}, brands to avoid, {min_review_score}, {min_review_count}.
    - Availability requirements: {shipping_region}, in-stock only or not.
    - Approximate {target_num_products} to compare (e.g., 5–10).
    - Any specific sites to prioritize or avoid.
    - Preferred table format ({output_table_format}: "csv" or "markdown").
    Ask the user to confirm or adjust these details. If any critical item is missing (e.g., unsure product type or budget), use
    "send_message" with a single focused question (e.g., "Do you have a maximum budget for {product_category}?").
    Once clarified, restate the final agreed criteria in a short "send_message" so they are recorded in the task history.
  validation_instruction: >
    Proceed only when:
    - {product_category} is clearly defined.
    - Budget constraints (or “no budget limit”) are explicitly stated.
    - There is at least a loose idea of required quality (e.g., "4.0+ rating" or "good overall reviews").
    - The user’s region or availability constraints are at least roughly understood.
    If anything remains ambiguous, use "send_message" again before searching.

  name: Discover candidate products via web search
  description: Use a small number of targeted web searches to collect URLs of promising product pages for the given product scope and criteria.
  action_instruction: >
    Use "google search" or "google search batch" with 1–3 well-formed queries, for example:
    - "{product_category} {use_case} best price {shipping_region}"
    - "{product_category} top rated {max_budget} {currency}"
    - "{product_category} review {year}"
    For each query:
      - Request a modest number of results (e.g., num_results: {search_limit_per_query} such as 5–10).
      - From returned search_results, filter to likely product detail pages (e.g., major retailers, comparison sites with clear product links).
    Collect the top {target_num_products}–{max_candidates} URLs that look like individual product pages.
    If there are more than needed, prefer pages:
      - From known, reputable retailers.
      - That mention price in the snippet.
      - That appear in the user’s preferred region/site list if given.
    If too few candidates are found, adjust one query (e.g., remove budget constraint) and run one more "google search".
  validation_instruction: >
    Check that:
    - You have at least {min_candidates} and at most {max_candidates} candidate URLs.
    - URLs appear to be product pages (based on titles/snippets like "Buy", "Price", "Reviews").
    If you cannot find enough candidates:
    - Use "send_message" to explain the limitation and ask the user if they want to relax constraints (budget, region, etc.) or continue with fewer products.

  name: Extract product details and build comparison table
  description: Visit each candidate product page, extract key information (name, price, rating, availability, basic attributes), and save a comparison table in the workspace.
  action_instruction: >
    Use a single "create and run python script" action to:
    - Hard-code or pass in:
      - The list of candidate URLs {product_urls}.
      - User criteria (e.g., {max_budget}, {min_review_score}, {min_review_count}).
      - Output options: {output_table_format}, {output_table_path}.
    - For each URL:
      - Fetch the page using Python standard libraries (e.g., urllib.request).
      - Extract:
        - product_name: from <title>, h1, or common patterns.
        - price text: search for currency symbols ({currency}) or patterns like "¥", "$", "€" plus numbers.
        - average_rating: look for patterns like "4.5 out of 5", "4.3/5", or stars text.
        - rating_count: look for patterns like "123 ratings" or "(123 reviews)".
        - availability_status: simple keyword checks like "In stock", "Out of stock", "Currently unavailable".
        - key_attributes: a short concatenated string from bullet points or a short spec list if easy to extract.
      - Convert price text into:
        - price_value (float or numeric) where possible.
        - price_currency (string), defaulting to {currency} if ambiguous.
      - Apply basic filters:
        - If price_value > {max_budget} (when budget is set), mark product as over_budget but still include in the table.
        - If average_rating or rating_count are missing, leave fields blank but keep the row.
      - Append a row with:
        - product_name, source_url, price_value, price_currency,
          average_rating, rating_count, availability_status, key_attributes.
    - After processing all URLs:
      - Write the rows to {output_table_path} as:
        - CSV if {output_table_format} == "csv", or
        - Markdown table if {output_table_format} == "markdown".
      - Optionally print a small summary to stdout:
        - How many products were parsed.
        - How many had valid prices and ratings.
  validation_instruction: >
    Check the script result:
    - Status should be success (no unhandled exception).
    - Confirm that {output_table_path} is mentioned in stdout or is as expected.
    - If very few products produced valid price/rating data, note this in a "send_message" to the user and continue, but be clear about the limitation.
    You do not need a second script pass; rely on basic sanity checks (e.g., no absurd prices) and move on.

  name: Analyze table and select best product
  description: Load the comparison table and pick the best product based on price, ratings, and user criteria in a simple, transparent way.
  action_instruction: >
    Use "create and run python script" again with a short script that:
    - Reads {output_table_path}.
    - Filters to "considered" products using simple rules:
      - If {max_budget} is set, prefer products with price_value <= {max_budget}, but keep over_budget ones for reference.
      - If {min_review_score} is set, filter for average_rating >= {min_review_score} when available.
      - If {min_review_count} is set, filter for rating_count >= {min_review_count} when available.
      - Optionally prefer availability_status containing phrases like "In stock".
    - For remaining products, compute a simple score such as:
      - score = (average_rating or default_rating) - α * normalized_price,
        where α is a small factor to reward lower prices. If some values are missing, handle them gracefully (e.g., treat missing rating as neutral but less preferred).
    - Rank products by score (descending), with tiebreakers:
      - Lower price.
      - Higher review count.
    - Select:
      - best_product (top-ranked),
      - optionally next_best_1 and next_best_2 as alternatives.
    - Print to stdout:
      - The chosen best product’s name, price, rating, and URL.
      - A short explanation of why it ranked highest (e.g., "highest rating within budget" or "best rating/price balance").
      - A brief overview of 1–2 strong runners-up if any.
  validation_instruction: >
    Ensure the script:
    - Produces at least one candidate in the final ranking; if not, fall back to the lowest-price product with valid data.
    - Prints a clearly identified best product and its basic details.
    If no reasonable candidate can be ranked (e.g., almost all fields missing), send a "send_message" to the user explaining that data was too sparse to choose confidently, but still mention any product with the most available information.

  name: Close task and report recommendation
  description: Present the results to the user, give them access to the table, and finalize the task status.
  action_instruction: >
    Use "send_message" to provide a concise, user-facing summary that includes:
    - Product scope ({product_category}, {use_case} if any).
    - Quick description of what was done (sites searched, roughly how many products inspected).
    - The recommended best product:
      - Name, approximate price {price_value} {price_currency}, average rating, link {source_url}, and notable attributes.
    - 1–2 strong alternatives with a short note on how they differ (e.g., cheaper but slightly lower rating, or more expensive but higher quality).
    - Any caveats (e.g., limited availability, sparse review data).
    - The path to the comparison table file {output_table_path} in the workspace.
    If the results and table look reasonable:
      - Call "mark task completed" with a brief message summarizing the recommendation and table location.
    If the user decides they want to adjust criteria and rerun instead:
      - Either repeat the workflow with new inputs, or, if they explicitly cancel, call "mark task cancel" with the reason.
    If extraction or ranking repeatedly failed and no meaningful recommendation is possible:
      - Call "mark task error" with a short explanation and point to any partial table file that may still be useful.
