name: Contact information extraction
description: Gather contact details such as names, emails, and phone numbers from company or directory web pages and save them into a structured file in the workspace.

goal/outcome:
  - Relevant web pages ({target_urls} or pages discovered from {query}) are fetched successfully.
  - Contact records are extracted into a structured dataset with, where available:
    - name
    - email
    - phone
    - role/title
    - organization
    - source_url
  - Obvious duplicates (same email or same name+phone from same source) are consolidated.
  - A single machine-readable output file (e.g., CSV or TSV) is created at {output_file_path} in the workspace.
  - A short summary is sent to the user with:
    - how many pages were scanned
    - how many contact records were extracted
    - where the output file is saved.

inputs_params:
  - Required: One of:
    - A list of specific URLs to scan: {target_urls}, or
    - A search query/queries like {company_name} or {directory_query} to find pages.
  - Required: Output file path and format:
    - e.g., {output_file_path} with extension .csv or .tsv.
  - Optional: Scope and type of contacts:
    - e.g., “people in {department}”, “sales contacts”, “offices in {region}”.
  - Optional: Field priorities:
    - which fields are most important (e.g., email is required; phone optional).
  - Optional: Max pages or results to process:
    - e.g., {max_pages}, {max_results_per_query}.
  - Optional: Domains or URL patterns to include/exclude:
    - include: {include_domains}, {include_path_pattern}
    - exclude: {exclude_domains}, {exclude_path_pattern}
  - Optional: Whether to include “generic” addresses like info@ or sales@:
    - include_generic_contacts: true/false.

context:
  reasoning:
    - Keep the workflow short: confirm scope, gather pages, extract contacts in one or two scripts, and write a single output file.
    - Prefer text-based HTTP and HTML extraction using:
      - "google search" or "google search batch" to discover pages from {query}, when needed.
      - "read web page from URL" to fetch and simplify page content.
      - "create and run python script" to parse text and extract contact details using regex and simple heuristics.
    - Avoid GUI mode; CLI and HTTP-based actions are sufficient.
    - Use simple, robust patterns to detect:
      - emails: basic email regex
      - phone numbers: simple patterns with digits and separators
      - names/roles: line-level heuristics near emails/phones where possible (not perfect but good enough).
    - Deduplicate primarily by email (and optionally by phone) to avoid repeated entries from multiple pages.
    - Log minimal metadata (e.g., source_url) per contact to preserve traceability.
  deadline: {time_left} left until deadline {deadline}
  definition_of_done(check during final validation step):
    - A structured file exists at {output_file_path} and contains at least a header row and zero or more contact rows.
    - Each row has at least:
      - email OR phone (preferably email) and
      - source_url.
    - No obvious malformed emails (e.g., missing '@') or empty contact rows.
    - The user has received a summary message with page count, contact count, and output path.
  avoid:
    - Over-engineering HTML parsing; simple regex over cleaned text is enough unless clearly insufficient.
    - Crawling the entire internet or many nested links; respect {max_pages} and domain constraints.
    - Using GUI mode for browsing.
    - Creating multiple scattered output files; use one main file.
    - Storing or outputting sensitive data beyond contact details on the relevant pages.

steps:
  name: Confirm extraction scope and output format
  description: Clarify where to get contact data from (URLs or queries) and what output file to create.
  action_instruction: >
    Use "send_message" to restate and confirm:
    - Whether the user will provide:
      - A list of URLs {target_urls}, or
      - A search query {directory_query} or {company_name} for discovery.
    - The desired output file path and format, e.g. {output_file_path} (default: contacts_{date_time}.csv).
    - Any scope constraints (e.g., “only from {company_domain}”, “only the {region} office page”).
    - Approximate limit on:
      - number of pages to process: {max_pages}
      - number of search results per query: {max_results_per_query}.
    - Whether generic emails like info@ or sales@ should be included.
    Ask the user to confirm or correct these. If a critical item is unclear (e.g. URLs vs query, or no output path),
    use "send_message" to ask a focused question. Once clarified, send a short "send_message" summarizing:
    - final list of URLs or search query,
    - main domain filters (if any),
    - output path {output_file_path}.
  validation_instruction: >
    Ensure that:
    - Either {target_urls} is non-empty OR {directory_query} / {company_name} is provided.
    - {output_file_path} is defined (or a default can be constructed).
    If not, use "send_message" again. Proceed only when the data source and output path are clear.

  name: Discover or finalize target URLs
  description: Produce the final list of URLs to process, either directly from the user or via a simple search.
  action_instruction: >
    If the user provided explicit URLs:
      - Use those as {final_urls} without extra discovery.
    If the user provided a search query instead:
      - Call "google search" or "google search batch" with:
        - query: {directory_query} or "{company_name} contacts" / "{company_name} directory"
        - num_results: {max_results_per_query} (or a small default like 5–10).
      - From the search results, collect URLs that:
        - match {include_domains} or the main company/domain, and
        - look like contact/directory pages (e.g., URL contains "contact", "team", "directory", "staff", "offices").
    Assemble the final URL list {final_urls}. If it is empty, send a short "send_message" explaining no useful pages were found
    and ask the user if they want to adjust the query or provide URLs manually.
  validation_instruction: >
    Confirm that {final_urls} is non-empty. If empty:
      - Inform the user with "send_message".
      - Ask if they want to try another query or stop.
    Proceed only when a non-empty {final_urls} has been agreed.

  name: Fetch pages and extract contacts
  description: For each target URL, fetch the page text and extract contact details into an in-memory list, then write them to a single output file.
  action_instruction: >
    Use "create and run python script" with a script that:
    - Has inputs embedded or easily configurable:
      - final_urls = {final_urls}
      - include_generic_contacts = {include_generic_contacts}
      - output_file_path = {output_file_path}
    - For each url in final_urls:
      - Call the "read web page from URL" action separately before the script if needed, OR
        within the script rely on Python's requests (only if allowed and credentials not needed).
      - Prefer using the existing "read web page from URL" action:
        - For each URL, call "read web page from URL" separately and pass:
          - url: {url}
          - timeout: {timeout_seconds} (e.g., 10–20)
          - extract_main: true
          - include_html: false
          - max_bytes: a reasonable limit (e.g., 1–2 MB)
        - Collect the "content" and "title" field per URL.
      - In the Python script, iterate over the fetched content strings and:
        - Normalize text (e.g., replace multiple spaces, remove obvious boilerplate if easy).
        - Use simple regular expressions to find:
          - emails: patterns like [A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}
          - phone numbers: patterns with digits and optional separators (e.g., +, -, spaces, parentheses).
        - For each email or phone, attempt to extract a nearby name/role:
          - e.g., previous line or same line segments near the email/phone.
        - Create contact entries with fields:
          - name (may be empty if not clear)
          - email
          - phone
          - role (optional, best-effort)
          - organization (if {company_name} is given, or possibly inferred from page title/domain)
          - source_url (the current URL).
    - Deduplicate contacts by:
      - key = email if present, otherwise phone + source_url.
    - Write all contacts to {output_file_path} as CSV/TSV with a header row.
    - Print to stdout:
      - number of URLs processed
      - number of raw matches (before dedupe)
      - number of final unique contacts.
  validation_instruction: >
    Check the action output:
    - Status must be success (no unhandled exceptions).
    - Ensure the script reports non-zero URLs processed (matching the length of {final_urls}).
    - Note the number of final unique contacts reported.
    If the script reports zero contacts:
      - Send a short "send_message" to the user explaining that no contacts were found from these pages
        and ask if they want to adjust inputs or accept the empty result.

  name: Validate extracted file and basic data quality
  description: Perform a quick sanity check on the generated contact file to ensure it is usable.
  action_instruction: >
    Use "shell view" or a small "create and run python script" to:
      - Open {output_file_path}.
      - Confirm it has:
        - a header row
        - correctly delimited columns
        - at least email or phone filled for rows (skip header).
      - Optionally count how many rows are present (excluding header).
    If severe issues are detected (e.g., file missing, format broken), explain briefly to the user
    using "send_message" and decide whether to re-run extraction or "mark task error".
  validation_instruction: >
    Confirm:
    - {output_file_path} exists.
    - At least the header row is present.
    - There is no obvious corruption (e.g., single giant line with no delimiters).
    If everything looks reasonable, proceed. If not, notify the user with "send_message" and consider re-running or ending with error.

  name: Close task and report to user
  description: Summarize the extraction results and mark the task status.
  action_instruction: >
    Use "send_message" to provide a short summary including:
    - Number of URLs processed (from {final_urls}).
    - Number of contacts found before and after deduplication.
    - Any notable patterns (e.g., mostly generic emails, few phone numbers).
    - The path to the output file {output_file_path} in the workspace.
    If the results are acceptable and the file was created:
      - Call "mark task completed" with a concise message summarizing counts and file path.
    If the user decided to stop early due to unsatisfactory pages or inputs:
      - Call "mark task cancel" with a brief reason.
    If extraction repeatedly fails (e.g., cannot fetch pages, file cannot be written) and a usable result
    cannot be produced, call "mark task error" with a short explanation and any partial output path if applicable.
