{
  "_id": { "$oid": "68b6ad213aa5a3e9d9026c7c" },
  "name": "batch_summarize_files",
  "description": "Summarizes all .txt, .md, and .docx files inside a directory using improved summarization logic with chunking and centroid-based clustering.",
  "type": "atomic",
  "mode": "CLI",
  "execution_mode": "sandboxed",
  "platforms": ["windows", "linux", "darwin"],
  "input_schema": {
    "directory_path": {
      "type": "string",
      "example": "/workspace/docs",
      "description": "Directory containing files to summarize."
    },
    "output_directory": {
      "type": "string",
      "example": "/workspace/summaries",
      "description": "Directory where summaries will be saved. Defaults to same folder."
    },
    "top_k": {
      "type": "integer",
      "example": 5,
      "description": "Number of clusters for summary.",
      "default": 5
    },
    "threshold": {
      "type": "number",
      "example": 0.55,
      "description": "Semantic similarity threshold.",
      "default": 0.55
    },
    "keywords": {
      "type": "array",
      "example": ["AI", "machine learning"],
      "description": "Optional keyword filters.",
      "default": []
    }
  },
  "output_schema": {
    "summaries": {
      "type": "array",
      "description": "List of {input, summary_file} for each processed file."
    }
  },
  "scope": ["global"],
  "code": "raise NotImplementedError('Platform overrides required')",
  "platform_overrides": {
    "linux": {
      "code": "import os, json, re, sys, importlib, subprocess, asyncio, concurrent.futures\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sentence_transformers import SentenceTransformer\nimport aiofiles\n\nasync def main():\n    directory = input_data.get('directory_path')\n    out_dir = input_data.get('output_directory') or directory\n    top_k = int(input_data.get('top_k', 5))\n    threshold = float(input_data.get('threshold', 0.55))\n    keywords = input_data.get('keywords') or []\n\n    if not directory or not os.path.isdir(directory):\n        raise ValueError('directory_path must be a valid directory')\n\n    os.makedirs(out_dir, exist_ok=True)\n\n    for pkg in ['scikit-learn', 'sentence-transformers', 'aiofiles', 'python-docx']:\n        try:\n            importlib.import_module(pkg.replace('-', '_'))\n        except:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\n    from docx import Document\n\n    def load_file(path):\n        ext = os.path.splitext(path)[1].lower()\n        if ext in ('.txt', '.md'):\n            with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                return f.read()\n        elif ext == '.docx':\n            doc = Document(path)\n            return '\\n'.join(p.text for p in doc.paragraphs)\n        return ''\n\n    supported = [os.path.join(directory, f) for f in os.listdir(directory)\n                 if os.path.splitext(f)[1].lower() in ('.txt', '.md', '.docx')]\n\n    if not supported:\n        output =(json.dumps({'summaries': []}))\n        return\n\n    summaries = []\n\n    for path in supported:\n        content = load_file(path)\n        if not content.strip():\n            continue\n\n        sentences = []\n        for paragraph in content.split('\\n'):\n            paragraph = paragraph.strip()\n            if not paragraph: continue\n            para_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', paragraph) if s.strip()]\n            for sent in para_sentences:\n                words = sent.split()\n                chunk_size = 50\n                for i in range(0, len(words), chunk_size):\n                    chunk = ' '.join(words[i:i+chunk_size])\n                    sentences.append(chunk)\n\n        if not sentences: continue\n\n        if keywords:\n            pattern = re.compile(r'\\\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\\\b', re.I)\n            filtered = [s for s in sentences if pattern.search(s)]\n            if filtered:\n                sentences = filtered\n\n        base = os.path.splitext(os.path.basename(path))[0]\n        out_file = os.path.join(out_dir, f\"{base}_summary.txt\")\n\n        loop = asyncio.get_running_loop()\n        with concurrent.futures.ThreadPoolExecutor() as pool:\n            await loop.run_in_executor(pool, lambda: summarize(sentences, top_k, threshold, out_file))\n\n        summaries.append({\"input\": os.path.basename(path), \"summary_file\": out_file})\n\n    output = (json.dumps({\"summaries\": summaries}))\n\n\ndef summarize(sentences, top_k, threshold, out_file):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(sentences, normalize_embeddings=True)\n\n    centroid = embeddings.mean(axis=0)\n    sims = embeddings @ centroid\n    idx = np.where(sims >= threshold)[0]\n    if len(idx) < max(3, top_k):\n        idx = list(range(len(sentences)))\n\n    filtered_s = [sentences[i] for i in idx]\n    filtered_e = embeddings[idx]\n\n    n_clusters = min(top_k, len(filtered_s))\n    km = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n    labels = km.fit_predict(filtered_e)\n\n    summary_s = []\n    for cluster_id in range(n_clusters):\n        cluster_idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n        cluster_emb = filtered_e[cluster_idxs]\n        cluster_centroid = cluster_emb.mean(axis=0)\n        closest_idx = cluster_idxs[np.argmax(cluster_emb @ cluster_centroid)]\n        summary_s.append(filtered_s[closest_idx])\n\n    final = ' '.join(re.sub(r'\\\\s+', ' ', s.strip()) for s in summary_s if s.strip())\n    asyncio.run(write_out(final, out_file))\n\n\nasync def write_out(text, path):\n    async with aiofiles.open(path, 'w', encoding='utf-8') as f:\n        await f.write(text)\n\n\nasyncio.run(main())"
    },
    "windows": {
      "code": "import os, json, re, sys, importlib, subprocess, asyncio, concurrent.futures\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sentence_transformers import SentenceTransformer\nimport aiofiles\n\nasync def main():\n    directory = input_data.get('directory_path')\n    out_dir = input_data.get('output_directory') or directory\n    top_k = int(input_data.get('top_k', 5))\n    threshold = float(input_data.get('threshold', 0.55))\n    keywords = input_data.get('keywords') or []\n\n    if not directory or not os.path.isdir(directory):\n        raise ValueError('directory_path must be a valid directory')\n\n    os.makedirs(out_dir, exist_ok=True)\n\n    for pkg in ['scikit-learn', 'sentence-transformers', 'aiofiles']:\n        try:\n            importlib.import_module(pkg.replace('-', '_'))\n        except:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\n    def load_file(path):\n        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n            return f.read()\n\n    supported = [os.path.join(directory, f) for f in os.listdir(directory)\n                 if os.path.splitext(f)[1].lower() in ('.txt', '.md')]\n\n    if not supported:\n        output = (json.dumps({'summaries': []}))\n        return\n\n    summaries = []\n\n    for path in supported:\n        content = load_file(path)\n        if not content.strip(): continue\n\n        sentences = []\n        for paragraph in content.split('\\n'):\n            paragraph = paragraph.strip()\n            if not paragraph: continue\n            para_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', paragraph) if s.strip()]\n            for sent in para_sentences:\n                words = sent.split()\n                chunk_size = 50\n                for i in range(0, len(words), chunk_size):\n                    chunk = ' '.join(words[i:i+chunk_size])\n                    sentences.append(chunk)\n\n        if not sentences: continue\n\n        if keywords:\n            pattern = re.compile(r'\\\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\\\b', re.I)\n            filtered = [s for s in sentences if pattern.search(s)]\n            if filtered:\n                sentences = filtered\n\n        base = os.path.splitext(os.path.basename(path))[0]\n        out_file = os.path.join(out_dir, f\"{base}_summary.txt\")\n\n        loop = asyncio.get_running_loop()\n        with concurrent.futures.ThreadPoolExecutor() as pool:\n            await loop.run_in_executor(pool, lambda: summarize(sentences, top_k, threshold, out_file))\n\n        summaries.append({\"input\": os.path.basename(path), \"summary_file\": out_file})\n\n    output = (json.dumps({\"summaries\": summaries}))\n\n\ndef summarize(sentences, top_k, threshold, out_file):\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(sentences, normalize_embeddings=True)\n\n    centroid = embeddings.mean(axis=0)\n    sims = embeddings @ centroid\n    idx = np.where(sims >= threshold)[0]\n    if len(idx) < max(3, top_k):\n        idx = list(range(len(sentences)))\n\n    filtered_s = [sentences[i] for i in idx]\n    filtered_e = embeddings[idx]\n\n    n_clusters = min(top_k, len(filtered_s))\n    km = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n    labels = km.fit_predict(filtered_e)\n\n    summary_s = []\n    for cluster_id in range(n_clusters):\n        cluster_idxs = [i for i, lbl in enumerate(labels) if lbl == cluster_id]\n        cluster_emb = filtered_e[cluster_idxs]\n        cluster_centroid = cluster_emb.mean(axis=0)\n        closest_idx = cluster_idxs[np.argmax(cluster_emb @ cluster_centroid)]\n        summary_s.append(filtered_s[closest_idx])\n\n    final = ' '.join(re.sub(r'\\\\s+', ' ', s.strip()) for s in summary_s if s.strip())\n    asyncio.run(write_out(final, out_file))\n\n\nasync def write_out(text, path):\n    async with aiofiles.open(path, 'w', encoding='utf-8') as f:\n        await f.write(text)\n\n\nasyncio.run(main())"
    },
    "darwin": {
      "code": "## macOS version uses the same code as the Linux override for consistency."
    }
  }
}
