{
  "_id": { "$oid": "68b6ad213aa5a3e9d9026b9a" },
  "name": "summarize file content",
  "description": "Reads a text file and write the summary to a new text file.",
  "type": "atomic",
  "execution_mode": "sandboxed",
  "mode": "CLI",
  "platforms": ["windows", "linux", "darwin"],
  "input_schema": {
    "input_file": { "type": "string", "example": "/path/to/input.txt", "description": "Path to the input text file to summarize." },
    "output_file": { "type": "string", "example": "/path/to/output_summary.txt", "description": "Path where the summary will be saved. Defaults to appending '_summary.txt' to input file.", "default": "" },
    "top_k": { "type": "integer", "example": 5, "description": "Number of clusters to form.", "default": 5 },
    "threshold": { "type": "number", "example": 0.55, "description": "Semantic similarity threshold for filtering sentences.", "default": 0.55 },
    "keywords": { "type": "array", "example": ["AI", "machine learning"], "description": "Optional keywords to filter sentences.", "default": [] }
  },
  "output_schema": {
    "summary_file": { "type": "string", "example": "/path/to/output_summary.txt", "description": "Path of the generated summary file." }
  },
  "scope": ["global"],
  "code": "raise NotImplementedError('Platform overrides are required')",
"platform_overrides": {
  "linux": {
    "code": "import os, json, re, sys, importlib, subprocess, asyncio, concurrent.futures\n\nasync def main():\n    input_file = input_data.get('input_file')\n    output_file = input_data.get('output_file')\n    if not input_file or not os.path.isfile(input_file):\n        raise ValueError('Input file must exist.')\n\n    if not output_file:\n        base, ext = os.path.splitext(input_file)\n        output_file = f'{base}_summary.txt'\n\n    keywords = input_data.get('keywords') or []\n    top_k = int(input_data.get('top_k', 5))\n    threshold = float(input_data.get('threshold', 0.55))\n\n    for pkg in ['scikit-learn', 'sentence-transformers', 'aiofiles']:\n        try:\n            importlib.import_module(pkg.replace('-', '_'))\n        except:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\n    import aiofiles\n\n    async with aiofiles.open(input_file, 'r', encoding='utf-8') as f:\n        content = await f.read()\n\n    segments = chunk_text(content, chunk_size=300, overlap=50)\n    if not segments:\n        async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:\n            await f.write('')\n        output = (json.dumps({'summary_file': output_file}))\n        return\n\n    if keywords:\n        pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.I)\n        filtered_segments = [s for s in segments if pattern.search(s['text'])]\n        if filtered_segments:\n            segments = filtered_segments\n\n    loop = asyncio.get_running_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        summary_file = await loop.run_in_executor(pool, lambda: process_summary(segments, top_k, threshold, output_file))\n\n    output = (json.dumps({'summary_file': summary_file}))\n\n\ndef chunk_text(text, chunk_size=300, overlap=50):\n    import re as _re\n    words = _re.findall(r'\\S+', text or '')\n    if not words:\n        return []\n    if chunk_size <= 0:\n        chunk_size = 300\n    if overlap < 0:\n        overlap = 0\n    step = max(1, chunk_size - overlap)\n    n = len(words)\n    segments = []\n    for start in range(0, n, step):\n        end = min(start + chunk_size, n)\n        chunk_words = words[start:end]\n        if not chunk_words:\n            break\n        chunk_text_val = ' '.join(chunk_words).strip()\n        if not chunk_text_val:\n            continue\n        has_leading = start > 0\n        has_trailing = end < n\n        segments.append({\n            'text': chunk_text_val,\n            'start_word_index': start + 1,\n            'has_leading_ellipsis': bool(has_leading),\n            'has_trailing_ellipsis': bool(has_trailing)\n        })\n    return segments\n\n\ndef process_summary(segments, top_k, threshold, output_file):\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import asyncio, aiofiles, re\n\n    if not segments:\n        asyncio.run(save_summary('', output_file))\n        return output_file\n\n    texts = [s['text'] for s in segments]\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(texts, normalize_embeddings=True)\n\n    centroid = embeddings.mean(axis=0)\n    sims = embeddings @ centroid\n    filtered_idx = np.where(sims >= threshold)[0]\n    if filtered_idx.size > 0:\n        filtered_segments = [segments[i] for i in filtered_idx]\n        filtered_embeddings = embeddings[filtered_idx]\n    else:\n        filtered_segments = segments\n        filtered_embeddings = embeddings\n\n    n_clusters = min(max(1, top_k), len(filtered_segments))\n    km = KMeans(n_clusters=n_clusters, n_init='auto')\n    labels = km.fit_predict(filtered_embeddings)\n    clusters = []\n    for cluster_id in range(km.n_clusters):\n        idx = np.where(labels == cluster_id)[0]\n        clusters.append([filtered_segments[i] for i in idx])\n\n    summary_segments = []\n    for cluster in clusters:\n        if not cluster:\n            continue\n        rep = cluster[len(cluster) // 2]\n        summary_segments.append(rep)\n\n    def clean_segment_text(s):\n        s = s.strip()\n        s = re.sub(r'\\s+', ' ', s)\n        return s\n\n    seen_texts = set()\n    final_paragraphs = []\n    for seg in summary_segments:\n        text_clean = clean_segment_text(seg['text'])\n        if not text_clean or text_clean in seen_texts:\n            continue\n        seen_texts.add(text_clean)\n        display_text = text_clean\n        if seg.get('has_leading_ellipsis'):\n            display_text = '...' + display_text\n        if seg.get('has_trailing_ellipsis'):\n            if not display_text.endswith('...'):\n                display_text = display_text + '...'\n        line_no = int(seg.get('start_word_index', 1))\n        para = f\"[line {line_no}] {display_text}\"\n        final_paragraphs.append(para)\n\n    final_summary = '\\n\\n'.join(final_paragraphs)\n\n    asyncio.run(save_summary(final_summary, output_file))\n    return output_file\n\nasync def save_summary(text, path):\n    import aiofiles\n    async with aiofiles.open(path, 'w', encoding='utf-8') as f:\n        await f.write(text)\n\nasyncio.run(main())"
  },
  "windows": {
    "code": "import os, json, re, sys, importlib, subprocess, asyncio, concurrent.futures\n\nasync def main():\n    input_file = input_data.get('input_file')\n    output_file = input_data.get('output_file')\n    if not input_file or not os.path.isfile(input_file):\n        raise ValueError('Input file must exist.')\n\n    if not output_file:\n        base, ext = os.path.splitext(input_file)\n        output_file = f'{base}_summary.txt'\n\n    keywords = input_data.get('keywords') or []\n    top_k = int(input_data.get('top_k', 5))\n    threshold = float(input_data.get('threshold', 0.55))\n\n    for pkg in ['scikit-learn', 'sentence-transformers', 'aiofiles']:\n        try:\n            importlib.import_module(pkg.replace('-', '_'))\n        except:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\n    import aiofiles\n\n    async with aiofiles.open(input_file, 'r', encoding='utf-8') as f:\n        content = await f.read()\n\n    segments = chunk_text(content, chunk_size=300, overlap=50)\n    if not segments:\n        async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:\n            await f.write('')\n        output = (json.dumps({'summary_file': output_file}))\n        return\n\n    if keywords:\n        pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.I)\n        filtered_segments = [s for s in segments if pattern.search(s['text'])]\n        if filtered_segments:\n            segments = filtered_segments\n\n    loop = asyncio.get_running_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        summary_file = await loop.run_in_executor(pool, lambda: process_summary(segments, top_k, threshold, output_file))\n\n    output = (json.dumps({'summary_file': summary_file}))\n\n\ndef chunk_text(text, chunk_size=300, overlap=50):\n    import re as _re\n    words = _re.findall(r'\\S+', text or '')\n    if not words:\n        return []\n    if chunk_size <= 0:\n        chunk_size = 300\n    if overlap < 0:\n        overlap = 0\n    step = max(1, chunk_size - overlap)\n    n = len(words)\n    segments = []\n    for start in range(0, n, step):\n        end = min(start + chunk_size, n)\n        chunk_words = words[start:end]\n        if not chunk_words:\n            break\n        chunk_text_val = ' '.join(chunk_words).strip()\n        if not chunk_text_val:\n            continue\n        has_leading = start > 0\n        has_trailing = end < n\n        segments.append({\n            'text': chunk_text_val,\n            'start_word_index': start + 1,\n            'has_leading_ellipsis': bool(has_leading),\n            'has_trailing_ellipsis': bool(has_trailing)\n        })\n    return segments\n\n\ndef process_summary(segments, top_k, threshold, output_file):\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import asyncio, aiofiles, re\n\n    if not segments:\n        asyncio.run(save_summary('', output_file))\n        return output_file\n\n    texts = [s['text'] for s in segments]\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(texts, normalize_embeddings=True)\n\n    centroid = embeddings.mean(axis=0)\n    sims = embeddings @ centroid\n    filtered_idx = np.where(sims >= threshold)[0]\n    if filtered_idx.size > 0:\n        filtered_segments = [segments[i] for i in filtered_idx]\n        filtered_embeddings = embeddings[filtered_idx]\n    else:\n        filtered_segments = segments\n        filtered_embeddings = embeddings\n\n    n_clusters = min(max(1, top_k), len(filtered_segments))\n    km = KMeans(n_clusters=n_clusters, n_init='auto')\n    labels = km.fit_predict(filtered_embeddings)\n    clusters = []\n    for cluster_id in range(km.n_clusters):\n        idx = np.where(labels == cluster_id)[0]\n        clusters.append([filtered_segments[i] for i in idx])\n\n    summary_segments = []\n    for cluster in clusters:\n        if not cluster:\n            continue\n        rep = cluster[len(cluster) // 2]\n        summary_segments.append(rep)\n\n    def clean_segment_text(s):\n        s = s.strip()\n        s = re.sub(r'\\s+', ' ', s)\n        return s\n\n    seen_texts = set()\n    final_paragraphs = []\n    for seg in summary_segments:\n        text_clean = clean_segment_text(seg['text'])\n        if not text_clean or text_clean in seen_texts:\n            continue\n        seen_texts.add(text_clean)\n        display_text = text_clean\n        if seg.get('has_leading_ellipsis'):\n            display_text = '...' + display_text\n        if seg.get('has_trailing_ellipsis'):\n            if not display_text.endswith('...'):\n                display_text = display_text + '...'\n        line_no = int(seg.get('start_word_index', 1))\n        para = f\"[line {line_no}] {display_text}\"\n        final_paragraphs.append(para)\n\n    final_summary = '\\n\\n'.join(final_paragraphs)\n\n    asyncio.run(save_summary(final_summary, output_file))\n    return output_file\n\nasync def save_summary(text, path):\n    import aiofiles\n    async with aiofiles.open(path, 'w', encoding='utf-8') as f:\n        await f.write(text)\n\nasyncio.run(main())"
  },
  "darwin": {
    "code": "import os, json, re, sys, importlib, subprocess, asyncio, concurrent.futures\n\nasync def main():\n    input_file = input_data.get('input_file')\n    output_file = input_data.get('output_file')\n    if not input_file or not os.path.isfile(input_file):\n        raise ValueError('Input file must exist.')\n\n    if not output_file:\n        base, ext = os.path.splitext(input_file)\n        output_file = f'{base}_summary.txt'\n\n    keywords = input_data.get('keywords') or []\n    top_k = int(input_data.get('top_k', 5))\n    threshold = float(input_data.get('threshold', 0.55))\n\n    for pkg in ['scikit-learn', 'sentence-transformers', 'aiofiles']:\n        try:\n            importlib.import_module(pkg.replace('-', '_'))\n        except:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '--quiet'])\n\n    import aiofiles\n\n    async with aiofiles.open(input_file, 'r', encoding='utf-8') as f:\n        content = await f.read()\n\n    segments = chunk_text(content, chunk_size=300, overlap=50)\n    if not segments:\n        async with aiofiles.open(output_file, 'w', encoding='utf-8') as f:\n            await f.write('')\n        output = (json.dumps({'summary_file': output_file}))\n        return\n\n    if keywords:\n        pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.I)\n        filtered_segments = [s for s in segments if pattern.search(s['text'])]\n        if filtered_segments:\n            segments = filtered_segments\n\n    loop = asyncio.get_running_loop()\n    with concurrent.futures.ThreadPoolExecutor() as pool:\n        summary_file = await loop.run_in_executor(pool, lambda: process_summary(segments, top_k, threshold, output_file))\n\n    output = (json.dumps({'summary_file': summary_file}))\n\n\ndef chunk_text(text, chunk_size=300, overlap=50):\n    import re as _re\n    words = _re.findall(r'\\S+', text or '')\n    if not words:\n        return []\n    if chunk_size <= 0:\n        chunk_size = 300\n    if overlap < 0:\n        overlap = 0\n    step = max(1, chunk_size - overlap)\n    n = len(words)\n    segments = []\n    for start in range(0, n, step):\n        end = min(start + chunk_size, n)\n        chunk_words = words[start:end]\n        if not chunk_words:\n            break\n        chunk_text_val = ' '.join(chunk_words).strip()\n        if not chunk_text_val:\n            continue\n        has_leading = start > 0\n        has_trailing = end < n\n        segments.append({\n            'text': chunk_text_val,\n            'start_word_index': start + 1,\n            'has_leading_ellipsis': bool(has_leading),\n            'has_trailing_ellipsis': bool(has_trailing)\n        })\n    return segments\n\n\ndef process_summary(segments, top_k, threshold, output_file):\n    from sentence_transformers import SentenceTransformer\n    import numpy as np\n    from sklearn.cluster import KMeans\n    import asyncio, aiofiles, re\n\n    if not segments:\n        asyncio.run(save_summary('', output_file))\n        return output_file\n\n    texts = [s['text'] for s in segments]\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    embeddings = model.encode(texts, normalize_embeddings=True)\n\n    centroid = embeddings.mean(axis=0)\n    sims = embeddings @ centroid\n    filtered_idx = np.where(sims >= threshold)[0]\n    if filtered_idx.size > 0:\n        filtered_segments = [segments[i] for i in filtered_idx]\n        filtered_embeddings = embeddings[filtered_idx]\n    else:\n        filtered_segments = segments\n        filtered_embeddings = embeddings\n\n    n_clusters = min(max(1, top_k), len(filtered_segments))\n    km = KMeans(n_clusters=n_clusters, n_init='auto')\n    labels = km.fit_predict(filtered_embeddings)\n    clusters = []\n    for cluster_id in range(km.n_clusters):\n        idx = np.where(labels == cluster_id)[0]\n        clusters.append([filtered_segments[i] for i in idx])\n\n    summary_segments = []\n    for cluster in clusters:\n        if not cluster:\n            continue\n        rep = cluster[len(cluster) // 2]\n        summary_segments.append(rep)\n\n    def clean_segment_text(s):\n        s = s.strip()\n        s = re.sub(r'\\s+', ' ', s)\n        return s\n\n    seen_texts = set()\n    final_paragraphs = []\n    for seg in summary_segments:\n        text_clean = clean_segment_text(seg['text'])\n        if not text_clean or text_clean in seen_texts:\n            continue\n        seen_texts.add(text_clean)\n        display_text = text_clean\n        if seg.get('has_leading_ellipsis'):\n            display_text = '...' + display_text\n        if seg.get('has_trailing_ellipsis'):\n            if not display_text.endswith('...'):\n                display_text = display_text + '...'\n        line_no = int(seg.get('start_word_index', 1))\n        para = f\"[line {line_no}] {display_text}\"\n        final_paragraphs.append(para)\n\n    final_summary = '\\n\\n'.join(final_paragraphs)\n\n    asyncio.run(save_summary(final_summary, output_file))\n    return output_file\n\nasync def save_summary(text, path):\n    import aiofiles\n    async with aiofiles.open(path, 'w', encoding='utf-8') as f:\n        await f.write(text)\n\nasyncio.run(main())"
  }
}
}
