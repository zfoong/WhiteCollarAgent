{
  "_id": {
    "$oid": "68b6c5333aa5a3e9d9026a20"
  },
  "name": "read web page from URL",
  "description": "Downloads a web page by URL and returns a clean, markdown-friendly text summary and title (no JavaScript execution).",
  "type": "atomic",
  "execution_mode": "sandboxed",
  "mode": "CLI",
  "input_schema": {
    "url": {
      "type": "string",
      "example": "https://example.com/article",
      "description": "The absolute URL of the page to fetch."
    },
    "timeout": {
      "type": "number",
      "example": 20,
      "description": "Request timeout in seconds."
    },
    "extract_main": {
      "type": "boolean",
      "example": true,
      "description": "If true, extract the main article content; otherwise return full-page text."
    },
    "include_html": {
      "type": "boolean",
      "example": false,
      "description": "If true, include the raw HTML in the output (truncated to max_bytes)."
    },
    "max_bytes": {
      "type": "integer",
      "example": 50000000,
      "description": "Maximum number of bytes to download before aborting."
    }
  },
  "output_schema": {
    "status": {
      "type": "string",
      "example": "success",
      "description": "'success' if the page was fetched and parsed, 'error' otherwise."
    },
    "final_url": {
      "type": "string",
      "example": "https://example.com/article",
      "description": "The final URL after redirects."
    },
    "title": {
      "type": "string",
      "example": "Example Article Title",
      "description": "The best-effort page title."
    },
    "content": {
      "type": "string",
      "example": "A concise, readable version of the page content in Markdown.",
      "description": "Markdown-friendly text extracted from the page."
    },
    "html": {
      "type": "string",
      "example": "<!doctype html><html>...</html>",
      "description": "Raw HTML if include_html=true; omitted otherwise."
    },
    "message": {
      "type": "string",
      "example": "Unsupported content-type.",
      "description": "Optional error or diagnostic message."
    }
  },
  "subActions": [],
  "default": false,
  "scope": [
    "global"
  ],
"code": "import json, re, sys, os, subprocess, importlib, requests\n\ndef _ensure(p):\n    try:\n        importlib.import_module(p)\n    except ImportError:\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install', p, '--quiet'])\n\nfor _p in ('trafilatura', 'beautifulsoup4', 'lxml'):\n    _ensure(_p)\n\nfrom bs4 import BeautifulSoup\nimport trafilatura\n\n\ndef main():\n    global output\n    url = str(input_data.get('url', '')).strip()\n    if not url or not re.match(r'^https?://', url, re.I):\n        output = json.dumps({'status': 'error', 'final_url': '', 'title': '', 'content': '', 'message': 'A valid http(s) URL is required.'})\n        return\n\n    timeout = float(input_data.get('timeout', 20))\n    extract_main = bool(input_data.get('extract_main', True))\n    include_html = bool(input_data.get('include_html', False))\n    max_bytes = int(input_data.get('max_bytes', 10_000_000))\n\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n        'Accept-Language': 'en-US,en;q=0.9'\n    }\n\n    final_url = url\n    data = b''\n    encoding = None\n\n    try:\n        with requests.get(url, headers=headers, timeout=timeout, allow_redirects=True, stream=True) as r:\n            r.raise_for_status()\n            final_url = str(r.url)\n            ctype = r.headers.get('Content-Type', '')\n            if not any(t in ctype for t in ('text/html', 'application/xhtml+xml', 'text/plain')):\n                output = json.dumps({'status': 'error', 'final_url': final_url, 'title': '', 'content': '', 'message': 'Unsupported content-type.'})\n                return\n            for chunk in r.iter_content(chunk_size=65536):\n                if chunk:\n                    data += chunk\n                    if len(data) > max_bytes:\n                        output = json.dumps({'status': 'error', 'final_url': final_url, 'title': '', 'content': '', 'message': 'Download exceeds max_bytes.'})\n                        return\n            encoding = r.encoding\n    except Exception as e:\n        output = json.dumps({'status': 'error', 'final_url': '', 'title': '', 'content': '', 'message': str(e)})\n        return\n\n    html_text = data.decode(encoding or 'utf-8', errors='replace') if data else ''\n\n    title = ''\n    content_md = ''\n\n    try:\n        if extract_main:\n            content_md = trafilatura.extract(data, url=final_url, include_comments=False, include_tables=True, output_format='markdown') or ''\n            try:\n                meta = trafilatura.metadata.extract_metadata(data, url=final_url)\n                if meta and getattr(meta, 'title', None):\n                    title = meta.title.strip()\n            except Exception:\n                pass\n        if not content_md:\n            soup = BeautifulSoup(html_text, 'lxml')\n            if not title:\n                t = soup.title.string.strip() if soup.title and soup.title.string else ''\n                title = t\n            for tag in soup(['script', 'style', 'noscript']):\n                tag.decompose()\n            txt = soup.get_text('\\n')\n            txt = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', txt)\n            content_md = txt.strip()\n    except Exception as e:\n        output = json.dumps({'status': 'error', 'final_url': final_url, 'title': '', 'content': '', 'message': str(e)})\n        return\n\n    out = {\n        'status': 'success',\n        'final_url': final_url,\n        'title': title or '',\n        'content': content_md or '',\n        'message': ''\n    }\n    if include_html:\n        out['html'] = html_text[:max_bytes]\n    output = json.dumps(out, ensure_ascii=False)\n\n\nmain()"
}
